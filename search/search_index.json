{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"ClusterBench","text":"<p>Welcome to the ClusterBench documentation.</p>"},{"location":"#overview","title":"Overview","text":"<p>A modular Python orchestrator for running containerized AI benchmarking workloads on HPC clusters via SLURM. This framework enables automated deployment, benchmarking, and monitoring of AI services including LLM inference servers, vector databases, in-memory stores, and relational databases on the MeluXina supercomputer.</p>"},{"location":"#architecture","title":"Architecture","text":"<pre><code>flowchart TB\n    subgraph Local[\"Local Machine\"]\n        direction LR\n        CLI[main.py] ~~~ Config[config.yaml]\n    end\n\n    subgraph Framework[\"Orchestrator Framework\"]\n        direction TB\n        Orch[Orchestrator]\n        subgraph Modules[\"Modules\"]\n            direction LR\n            Srv[Servers] ~~~ Cli[Clients] ~~~ Mon[Monitors]\n        end\n        Orch --&gt; Modules\n        Modules --&gt; SSH[SSHClient]\n    end\n\n    subgraph HPC[\"MeluXina HPC\"]\n        direction TB\n        SLURM[SLURM] --&gt; Compute[Compute] --&gt; Containers[Apptainer]\n    end\n\n    subgraph Services[\"Deployed Services\"]\n        direction LR\n        Ollama[Ollama] ~~~ Redis[Redis] ~~~ Chroma[Chroma] ~~~ MySQL[MySQL]\n    end\n\n    subgraph Monitoring[\"Monitoring Stack\"]\n        direction LR\n        cAdvisor[cAdvisor] --&gt; Prometheus[Prometheus] --&gt; Grafana[Grafana]\n    end\n\n    Local --&gt; Framework\n    Framework --&gt; HPC\n    HPC --&gt; Services\n    HPC --&gt; Monitoring\n\n    style CLI fill:#1976D2,color:#fff\n    style Config fill:#1976D2,color:#fff\n    style Orch fill:#388E3C,color:#fff\n    style Srv fill:#388E3C,color:#fff\n    style Cli fill:#388E3C,color:#fff\n    style Mon fill:#388E3C,color:#fff\n    style SSH fill:#455A64,color:#fff\n    style SLURM fill:#F57C00,color:#fff\n    style Compute fill:#F57C00,color:#fff\n    style Containers fill:#F57C00,color:#fff\n    style Ollama fill:#0288D1,color:#fff\n    style Redis fill:#D32F2F,color:#fff\n    style Chroma fill:#689F38,color:#fff\n    style MySQL fill:#1565C0,color:#fff\n    style cAdvisor fill:#7B1FA2,color:#fff\n    style Prometheus fill:#E64A19,color:#fff\n    style Grafana fill:#F57C00,color:#fff</code></pre>"},{"location":"#supported-services","title":"Supported Services","text":"Service Type Port Description Ollama LLM Inference 11434 High-performance LLM inference server Redis In-Memory DB 6379 Key-value store with persistence Chroma Vector DB 8000 Vector similarity search MySQL RDBMS 3306 Relational database Prometheus Monitoring 9090 Metrics collection Grafana Visualization 3000 Real-time dashboards"},{"location":"#quick-start","title":"Quick Start","text":"<pre><code># Install dependencies\npip install -r requirements.txt\n\n# Start an Ollama service\npython main.py --recipe recipes/services/ollama.yaml\n\n# Check status\npython main.py --status\n\n# Run benchmark client\npython main.py --recipe recipes/clients/ollama_benchmark.yaml --target-service &lt;SERVICE_ID&gt;\n\n# View results in Grafana (after SSH tunnel)\nopen http://localhost:3000\n</code></pre>"},{"location":"#documentation","title":"Documentation","text":"Getting Started Setup, installation, and your first benchmark Architecture System design, components, and data flow Services Ollama, Redis, Chroma, MySQL configuration CLI Reference Complete command-line interface documentation Recipes YAML configuration for services and clients Monitoring Grafana dashboards and Prometheus metrics"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>YAML-based Configuration: Define services and benchmarks declaratively</li> <li>Multi-Service Support: Ollama, Redis, Chroma, MySQL, and more</li> <li>Automated SLURM Integration: Seamless job submission and management</li> <li>Real-time Monitoring: Grafana dashboards with cAdvisor metrics</li> <li>Parametric Benchmarking: Sweep across multiple configurations</li> <li>SSH Tunneling: Secure access to HPC services</li> </ul>"},{"location":"#project-status","title":"Project Status","text":"<p>Version: 1.0.0 Last Updated: January 2026 Project: EUMaster4HPC Challenge 2025-2026 Supervisor: Dr. Farouk Mansouri Platform: MeluXina Supercomputer</p> <p>Built for the Software Atelier course in collaboration with EUMASTER4HPC and LuxProvide</p>"},{"location":"architecture/jobs/","title":"Job Hierarchy","text":""},{"location":"architecture/jobs/#overview","title":"Overview","text":"<p>The orchestrator uses a hierarchical job system where all deployable units inherit from a common base class. This enables consistent SLURM script generation while allowing service-specific customization.</p>"},{"location":"architecture/jobs/#job-class-hierarchy","title":"Job Class Hierarchy","text":"<pre><code>classDiagram\n    class BaseJob {\n        &lt;&lt;abstract&gt;&gt;\n        +name: str\n        +job_id: str\n        +config: dict\n        +resources: ResourceConfig\n        +environment: dict\n        +generate_slurm_script() str\n        +get_setup_commands()* list\n        +get_container_command()* str\n        +get_health_check_commands() list\n    }\n\n    class Service {\n        +ports: list[int]\n        +enable_cadvisor: bool\n        +cadvisor_port: int\n        +get_service_setup_commands() list\n        +get_cadvisor_commands() list\n    }\n\n    class Client {\n        +target_endpoint: str\n        +target_service_id: str\n        +duration: int\n        +get_benchmark_commands() list\n        +get_results_collection() list\n    }\n\n    class OllamaService {\n        +model: str\n        +gpu_layers: int\n        +get_model_pull_commands() list\n    }\n\n    class OllamaClient {\n        +num_requests: int\n        +concurrent: int\n        +prompts: list\n    }\n\n    class RedisService {\n        +persistence: str\n        +maxmemory: str\n    }\n\n    class RedisClient {\n        +clients: int\n        +data_size: int\n        +pipeline: int\n    }\n\n    class ChromaService {\n        +persist_directory: str\n    }\n\n    class MySQLService {\n        +root_password: str\n        +database: str\n    }\n\n    class PrometheusService {\n        +monitoring_targets: list\n        +retention_time: str\n    }\n\n    class GrafanaService {\n        +dashboards: list\n        +datasources: list\n    }\n\n    BaseJob &lt;|-- Service\n    BaseJob &lt;|-- Client\n    Service &lt;|-- OllamaService\n    Service &lt;|-- RedisService\n    Service &lt;|-- ChromaService\n    Service &lt;|-- MySQLService\n    Service &lt;|-- PrometheusService\n    Service &lt;|-- GrafanaService\n    Client &lt;|-- OllamaClient\n    Client &lt;|-- RedisClient\n    Client &lt;|-- ChromaClient\n    Client &lt;|-- MySQLClient</code></pre>"},{"location":"architecture/jobs/#slurm-script-generation","title":"SLURM Script Generation","text":"<p>Each job generates a complete SLURM batch script following this template:</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=ollama_abc123\n#SBATCH --account=p200981\n#SBATCH --partition=gpu\n#SBATCH --qos=default\n#SBATCH --nodes=1\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=4\n#SBATCH --mem=16G\n#SBATCH --time=02:00:00\n#SBATCH --gres=gpu:1\n#SBATCH --output=slurm-%j.out\n#SBATCH --error=slurm-%j.err\n\n# Environment setup\nmodule purge\nmodule load Apptainer/1.2.4-GCCcore-12.3.0\n\n# Service-specific setup (from get_setup_commands)\nexport OLLAMA_HOST=0.0.0.0:11434\nexport OLLAMA_MODELS=$HOME/.ollama/models\n\n# Container execution (from get_container_command)\napptainer exec --nv \\\n    --bind $HOME/.ollama:/root/.ollama \\\n    $HOME/containers/ollama_latest.sif \\\n    ollama serve &amp;\n\n# Health check (from get_health_check_commands)\nsleep 10\ncurl -s http://localhost:11434/api/tags\n\n# Keep running\nwait\n</code></pre>"},{"location":"architecture/jobs/#script-generation-methods","title":"Script Generation Methods","text":""},{"location":"architecture/jobs/#generate_slurm_script","title":"<code>generate_slurm_script()</code>","text":"<p>The main template method that assembles the complete script:</p> <pre><code>def generate_slurm_script(self) -&gt; str:\n    script = []\n    script.append(\"#!/bin/bash\")\n    script.extend(self._generate_slurm_header())\n    script.extend(self._generate_module_loads())\n    script.extend(self.get_setup_commands())        # Hook\n    script.extend(self.get_container_command())     # Hook\n    script.extend(self.get_health_check_commands()) # Hook\n    return '\\n'.join(script)\n</code></pre>"},{"location":"architecture/jobs/#get_setup_commands","title":"<code>get_setup_commands()</code>","text":"<p>Service-specific environment and preparation:</p> OllamaRedisPrometheus <pre><code>def get_setup_commands(self):\n    return [\n        \"export OLLAMA_HOST=0.0.0.0:11434\",\n        \"export OLLAMA_MODELS=$HOME/.ollama/models\",\n        \"mkdir -p $HOME/.ollama/models\",\n    ]\n</code></pre> <pre><code>def get_setup_commands(self):\n    return [\n        \"export REDIS_PORT=6379\",\n        \"mkdir -p $HOME/redis/data\",\n        \"mkdir -p $HOME/redis/logs\",\n    ]\n</code></pre> <pre><code>def get_setup_commands(self):\n    return [\n        \"mkdir -p $HOME/prometheus/data\",\n        \"mkdir -p $HOME/prometheus/config\",\n        \"cat &gt; $HOME/prometheus/config/prometheus.yml &lt;&lt; EOF\",\n        \"...\",\n        \"EOF\",\n    ]\n</code></pre>"},{"location":"architecture/jobs/#get_container_command","title":"<code>get_container_command()</code>","text":"<p>The Apptainer execution command:</p> Ollama (GPU)Redis (CPU) <pre><code>def get_container_command(self):\n    return \"\"\"\n    apptainer exec --nv \\\\\n        --bind $HOME/.ollama:/root/.ollama \\\\\n        $HOME/containers/ollama_latest.sif \\\\\n        ollama serve &amp;\n    \"\"\"\n</code></pre> <pre><code>def get_container_command(self):\n    return \"\"\"\n    apptainer exec \\\\\n        --bind $HOME/redis/data:/data \\\\\n        $HOME/containers/redis_latest.sif \\\\\n        redis-server --bind 0.0.0.0 --port 6379 &amp;\n    \"\"\"\n</code></pre>"},{"location":"architecture/jobs/#resource-configuration","title":"Resource Configuration","text":"<p>Resources are specified in recipes and mapped to SLURM directives:</p> Recipe Field SLURM Directive Description <code>nodes</code> <code>--nodes</code> Number of nodes <code>ntasks</code> <code>--ntasks</code> Number of tasks <code>cpus_per_task</code> <code>--cpus-per-task</code> CPUs per task <code>mem</code> <code>--mem</code> Memory allocation <code>time</code> <code>--time</code> Time limit <code>partition</code> <code>--partition</code> SLURM partition <code>qos</code> <code>--qos</code> Quality of service <code>gres</code> <code>--gres</code> Generic resources (GPU) <p>Example recipe resources:</p> <pre><code>resources:\n  nodes: 1\n  ntasks: 1\n  cpus_per_task: 4\n  mem: \"16G\"\n  time: \"02:00:00\"\n  partition: gpu\n  qos: default\n  gres: \"gpu:1\"\n</code></pre>"},{"location":"architecture/jobs/#cadvisor-integration","title":"cAdvisor Integration","text":"<p>Services can enable cAdvisor for container monitoring:</p> <pre><code>service:\n  name: ollama\n  enable_cadvisor: true\n  cadvisor_port: 8080\n</code></pre> <p>When enabled, the generated script includes:</p> <pre><code># Start cAdvisor sidecar\napptainer exec \\\n    --bind /sys:/sys:ro \\\n    --bind /var/lib/docker:/var/lib/docker:ro \\\n    $HOME/containers/cadvisor.sif \\\n    /usr/bin/cadvisor \\\n    --port=8080 \\\n    --docker_only=true &amp;\n</code></pre>"},{"location":"architecture/jobs/#job-lifecycle","title":"Job Lifecycle","text":"<pre><code>stateDiagram-v2\n    [*] --&gt; Created: Recipe loaded\n    Created --&gt; Submitted: sbatch\n    Submitted --&gt; Pending: In SLURM queue\n    Pending --&gt; Running: Resources allocated\n    Running --&gt; Completed: Job finished\n    Running --&gt; Failed: Error occurred\n    Running --&gt; Cancelled: scancel\n    Completed --&gt; [*]\n    Failed --&gt; [*]\n    Cancelled --&gt; [*]</code></pre> <p>Next: Services Overview</p>"},{"location":"architecture/overview/","title":"System Architecture","text":""},{"location":"architecture/overview/#overview","title":"Overview","text":"<p>The HPC AI Benchmarking Orchestrator uses a modular architecture designed for flexibility, extensibility, and seamless HPC integration. The system orchestrates containerized AI workloads through SLURM, providing automated deployment, monitoring, and benchmarking capabilities.</p>"},{"location":"architecture/overview/#high-level-architecture","title":"High-Level Architecture","text":"<pre><code>flowchart TB\n    subgraph Local[\"Local Machine\"]\n        direction LR\n        CLI[main.py] ~~~ Config[config.yaml] ~~~ Recipes[Recipes]\n    end\n\n    subgraph Framework[\"Core Framework\"]\n        direction TB\n        Orch[Orchestrator]\n        subgraph Modules[\"Modules\"]\n            direction LR\n            Srv[Servers] ~~~ Cli[Clients] ~~~ Mon[Monitors]\n        end\n        Orch --&gt; Modules\n        Modules --&gt; Factory[Factory]\n        Factory --&gt; SSH[SSH]\n    end\n\n    subgraph HPC[\"MeluXina HPC\"]\n        direction LR\n        SLURM[SLURM] --&gt; Compute[Compute] --&gt; Containers[Apptainer]\n    end\n\n    subgraph Services[\"Deployed Services\"]\n        direction LR\n        Ollama[Ollama] ~~~ Redis[Redis] ~~~ Chroma[Chroma] ~~~ MySQL[MySQL]\n    end\n\n    subgraph Monitoring[\"Monitoring Stack\"]\n        direction LR\n        cAdvisor[cAdvisor] --&gt; Prometheus[Prometheus] --&gt; Grafana[Grafana]\n    end\n\n    Local --&gt; Framework\n    Framework --&gt; HPC\n    HPC --&gt; Services\n    HPC --&gt; Monitoring\n\n    style CLI fill:#1976D2,color:#fff\n    style Config fill:#1976D2,color:#fff\n    style Recipes fill:#1976D2,color:#fff\n    style Orch fill:#388E3C,color:#fff\n    style Srv fill:#388E3C,color:#fff\n    style Cli fill:#388E3C,color:#fff\n    style Mon fill:#388E3C,color:#fff\n    style Factory fill:#F57C00,color:#fff\n    style SSH fill:#455A64,color:#fff\n    style SLURM fill:#7B1FA2,color:#fff\n    style Compute fill:#7B1FA2,color:#fff\n    style Containers fill:#7B1FA2,color:#fff\n    style Ollama fill:#0288D1,color:#fff\n    style Redis fill:#D32F2F,color:#fff\n    style Chroma fill:#689F38,color:#fff\n    style MySQL fill:#1565C0,color:#fff\n    style cAdvisor fill:#7B1FA2,color:#fff\n    style Prometheus fill:#E64A19,color:#fff\n    style Grafana fill:#F57C00,color:#fff</code></pre>"},{"location":"architecture/overview/#core-components","title":"Core Components","text":""},{"location":"architecture/overview/#1-cli-interface-mainpy","title":"1. CLI Interface (<code>main.py</code>)","text":"<p>The command-line interface provides the primary user interaction point.</p> <p>Responsibilities:</p> <ul> <li>Parse command-line arguments</li> <li>Load and validate configuration</li> <li>Route commands to appropriate modules</li> <li>Display status and results</li> </ul>"},{"location":"architecture/overview/#2-benchmarkorchestrator","title":"2. BenchmarkOrchestrator","text":"<p>The central coordination hub managing all operations.</p> <pre><code>classDiagram\n    class BenchmarkOrchestrator {\n        +config: dict\n        +ssh_client: SSHClient\n        +servers: ServersModule\n        +clients: ClientsModule\n        +monitors: MonitorsModule\n        +load_recipe(path) Recipe\n        +start_service(recipe) str\n        +start_client(recipe, target) str\n        +stop_service(service_id) bool\n        +show_status() dict\n    }\n\n    class ServersModule {\n        +start_service(recipe) ServiceInfo\n        +stop_service(service_id) bool\n        +list_services() list\n        +get_service_host(service_id) str\n    }\n\n    class ClientsModule {\n        +start_client(recipe, target) ClientInfo\n        +list_clients() list\n    }\n\n    class MonitorsModule {\n        +start_monitor(recipe) MonitorInfo\n        +query_metrics(query) dict\n    }\n\n    BenchmarkOrchestrator --&gt; ServersModule\n    BenchmarkOrchestrator --&gt; ClientsModule\n    BenchmarkOrchestrator --&gt; MonitorsModule</code></pre>"},{"location":"architecture/overview/#3-job-factory-service-classes","title":"3. Job Factory &amp; Service Classes","text":"<p>The Factory pattern enables extensible service/client creation.</p> <pre><code>classDiagram\n    class JobFactory {\n        +_services: dict\n        +_clients: dict\n        +register_service(name, cls)\n        +register_client(name, cls)\n        +create_service(recipe) Service\n        +create_client(recipe) Client\n    }\n\n    class BaseJob {\n        &lt;&lt;abstract&gt;&gt;\n        +name: str\n        +job_id: str\n        +generate_slurm_script() str\n        +get_setup_commands() list\n        +get_container_command() str\n    }\n\n    class Service {\n        +ports: list\n        +enable_cadvisor: bool\n        +get_service_setup_commands() list\n    }\n\n    class Client {\n        +target_endpoint: str\n        +get_benchmark_commands() list\n    }\n\n    class OllamaService {\n        +model: str\n        +get_model_pull_commands() list\n    }\n\n    class RedisService {\n        +persistence: str\n        +get_redis_config() list\n    }\n\n    BaseJob &lt;|-- Service\n    BaseJob &lt;|-- Client\n    Service &lt;|-- OllamaService\n    Service &lt;|-- RedisService\n    Service &lt;|-- ChromaService\n    Service &lt;|-- MySQLService\n    Service &lt;|-- PrometheusService\n    Service &lt;|-- GrafanaService\n    JobFactory ..&gt; BaseJob : creates</code></pre>"},{"location":"architecture/overview/#4-ssh-client","title":"4. SSH Client","text":"<p>Handles all remote HPC operations securely.</p> <p>Operations:</p> <ul> <li>Command execution via SSH</li> <li>File transfer (upload/download)</li> <li>Job submission (<code>sbatch</code>)</li> <li>Job management (<code>squeue</code>, <code>scancel</code>)</li> </ul>"},{"location":"architecture/overview/#5-slurm-integration","title":"5. SLURM Integration","text":"<p>Jobs are submitted as SLURM batch scripts with:</p> <ul> <li>Resource allocation (CPU, GPU, memory, time)</li> <li>Module loading (Apptainer)</li> <li>Container execution</li> <li>Output logging</li> </ul>"},{"location":"architecture/overview/#data-flow","title":"Data Flow","text":""},{"location":"architecture/overview/#service-deployment-flow","title":"Service Deployment Flow","text":"<pre><code>sequenceDiagram\n    participant User\n    participant CLI\n    participant Orchestrator\n    participant Factory\n    participant SSH\n    participant SLURM\n    participant Container\n\n    User-&gt;&gt;CLI: --recipe service.yaml\n    CLI-&gt;&gt;Orchestrator: start_service(recipe)\n    Orchestrator-&gt;&gt;Factory: create_service(recipe)\n    Factory--&gt;&gt;Orchestrator: ServiceInstance\n    Orchestrator-&gt;&gt;SSH: submit_job(script)\n    SSH-&gt;&gt;SLURM: sbatch script.sh\n    SLURM-&gt;&gt;Container: allocate &amp; run\n    Container--&gt;&gt;SLURM: running on node\n    SLURM--&gt;&gt;SSH: job_id, node\n    SSH--&gt;&gt;Orchestrator: ServiceInfo\n    Orchestrator--&gt;&gt;CLI: service_id\n    CLI--&gt;&gt;User: \"Service started: abc123\"</code></pre>"},{"location":"architecture/overview/#metrics-collection-flow","title":"Metrics Collection Flow","text":"<pre><code>sequenceDiagram\n    participant Service as Service Container\n    participant cAdvisor\n    participant Prometheus\n    participant Grafana\n    participant User\n\n    loop Every 15s\n        cAdvisor-&gt;&gt;Service: Collect container metrics\n        Service--&gt;&gt;cAdvisor: CPU, Memory, Network, Disk\n        Prometheus-&gt;&gt;cAdvisor: Scrape /metrics\n        cAdvisor--&gt;&gt;Prometheus: Metrics data\n    end\n\n    User-&gt;&gt;Grafana: Open dashboard\n    Grafana-&gt;&gt;Prometheus: PromQL query\n    Prometheus--&gt;&gt;Grafana: Time series data\n    Grafana--&gt;&gt;User: Visualizations</code></pre>"},{"location":"architecture/overview/#design-patterns","title":"Design Patterns","text":""},{"location":"architecture/overview/#factory-pattern","title":"Factory Pattern","text":"<p>Used for creating services and clients from YAML recipes:</p> <pre><code># Registration\nJobFactory.register_service('ollama', OllamaService)\nJobFactory.register_client('ollama_benchmark', OllamaBenchmarkClient)\n\n# Creation\nservice = JobFactory.create_service(recipe)  # Returns OllamaService\nclient = JobFactory.create_client(recipe)    # Returns OllamaBenchmarkClient\n</code></pre>"},{"location":"architecture/overview/#template-method-pattern","title":"Template Method Pattern","text":"<p>Base classes define the structure, subclasses customize behavior:</p> <pre><code>class BaseJob:\n    def generate_slurm_script(self):\n        script = []\n        script.extend(self._generate_header())      # Template\n        script.extend(self.get_setup_commands())    # Hook - override in subclass\n        script.extend(self._generate_execution())   # Template\n        return '\\n'.join(script)\n</code></pre>"},{"location":"architecture/overview/#strategy-pattern","title":"Strategy Pattern","text":"<p>Different services implement different setup strategies:</p> <pre><code>class OllamaService(Service):\n    def get_setup_commands(self):\n        return [\"export OLLAMA_HOST=0.0.0.0:11434\", ...]\n\nclass RedisService(Service):\n    def get_setup_commands(self):\n        return [\"redis-server --bind 0.0.0.0\", ...]\n</code></pre>"},{"location":"architecture/overview/#file-organization","title":"File Organization","text":"<pre><code>src/\n\u251c\u2500\u2500 orchestrator.py      # BenchmarkOrchestrator\n\u251c\u2500\u2500 servers.py           # ServersModule\n\u251c\u2500\u2500 clients.py           # ClientsModule\n\u251c\u2500\u2500 monitors.py          # MonitorsModule\n\u251c\u2500\u2500 ssh_client.py        # SSHClient\n\u251c\u2500\u2500 base.py              # BaseJob, enums, utilities\n\u2514\u2500\u2500 services/\n    \u251c\u2500\u2500 __init__.py      # JobFactory registration\n    \u251c\u2500\u2500 base.py          # Service/Client base classes\n    \u251c\u2500\u2500 ollama.py        # OllamaService, OllamaClient\n    \u251c\u2500\u2500 redis.py         # RedisService, RedisClient\n    \u251c\u2500\u2500 chroma.py        # ChromaService, ChromaClient\n    \u251c\u2500\u2500 mysql.py         # MySQLService, MySQLClient\n    \u251c\u2500\u2500 prometheus.py    # PrometheusService\n    \u2514\u2500\u2500 grafana.py       # GrafanaService\n</code></pre> <p>Next: Job Hierarchy | Services</p>"},{"location":"cli/commands/","title":"CLI Commands Reference","text":"<p>Complete reference for all command-line interface commands.</p>"},{"location":"cli/commands/#synopsis","title":"Synopsis","text":"<pre><code>python main.py [OPTIONS]\n</code></pre>"},{"location":"cli/commands/#global-options","title":"Global Options","text":"Option Description <code>--help</code>, <code>-h</code> Show help message and exit <code>--verbose</code>, <code>-v</code> Enable verbose logging output <code>--config CONFIG</code> Path to config file (default: <code>config.yaml</code>)"},{"location":"cli/commands/#service-management","title":"Service Management","text":""},{"location":"cli/commands/#-recipe-recipe","title":"<code>--recipe RECIPE</code>","text":"<p>Start a service or client from a YAML recipe file.</p> <pre><code>python main.py --recipe recipes/services/ollama.yaml\npython main.py --recipe recipes/clients/redis_benchmark.yaml --target-service redis_abc123\n</code></pre> <p>Output: <pre><code>Service started: ollama_a1b2c3d4\nMonitor the job status through SLURM or check logs.\n\n  To check status:\n    python main.py --status\n</code></pre></p>"},{"location":"cli/commands/#-target-service-service_id","title":"<code>--target-service SERVICE_ID</code>","text":"<p>Specify the target service for a client recipe. The orchestrator resolves the service's node and port automatically.</p> <pre><code>python main.py --recipe recipes/clients/ollama_benchmark.yaml --target-service ollama_abc123\n</code></pre>"},{"location":"cli/commands/#-target-endpoint-url","title":"<code>--target-endpoint URL</code>","text":"<p>Specify the target endpoint directly (alternative to <code>--target-service</code>).</p> <pre><code>python main.py --recipe recipes/clients/ollama_benchmark.yaml --target-endpoint http://mel2073:11434\n</code></pre>"},{"location":"cli/commands/#-stop-service-service_id","title":"<code>--stop-service SERVICE_ID</code>","text":"<p>Stop a specific running service by its ID or SLURM job ID.</p> <pre><code>python main.py --stop-service ollama_abc123\npython main.py --stop-service 3656789\n</code></pre> <p>Output: <pre><code>Successfully cancelled SLURM job: 3656789\nService ollama_abc123 stopped.\n</code></pre></p>"},{"location":"cli/commands/#-stop-all-services","title":"<code>--stop-all-services</code>","text":"<p>Stop all running services at once.</p> <pre><code>python main.py --stop-all-services\n</code></pre> <p>Output: <pre><code>Stopping all running services...\n\u2705 Stopped 4/4 services\n</code></pre></p>"},{"location":"cli/commands/#status-information","title":"Status &amp; Information","text":""},{"location":"cli/commands/#-status","title":"<code>--status</code>","text":"<p>Show status of all running SLURM jobs (services and clients).</p> <pre><code>python main.py --status\n</code></pre> <p>Output: <pre><code>SLURM Job Status:\n  Total Jobs: 3\n  Services: 2\n  Clients: 1\n  Other: 0\n\nServices:\n  JOB_ID  | SERVICE_ID        | STATUS  | RUNTIME  | NODE\n  3656789 | ollama_a1b2c3d4   | RUNNING | 0:15:30  | mel2073\n  3656790 | redis_e5f6g7h8    | RUNNING | 0:10:15  | mel0182\n\nClients:\n  JOB_ID  | CLIENT_ID              | STATUS  | RUNTIME  | NODE\n  3656791 | ollama_bench_i9j0k1l2  | RUNNING | 0:05:45  | mel2074\n</code></pre></p>"},{"location":"cli/commands/#-list-services","title":"<code>--list-services</code>","text":"<p>List all available service types.</p> <pre><code>python main.py --list-services\n</code></pre> <p>Output: <pre><code>Available Services:\n  - ollama          LLM inference server\n  - redis           In-memory database\n  - chroma          Vector database\n  - mysql           Relational database\n  - prometheus      Metrics collection\n  - grafana         Visualization dashboard\n</code></pre></p>"},{"location":"cli/commands/#-list-clients","title":"<code>--list-clients</code>","text":"<p>List all available client/benchmark types.</p> <pre><code>python main.py --list-clients\n</code></pre> <p>Output: <pre><code>Available Clients:\n  - ollama_benchmark      LLM inference benchmark\n  - redis_benchmark       Redis performance test\n  - chroma_benchmark      Vector DB benchmark\n  - mysql_benchmark       MySQL CRUD benchmark\n</code></pre></p>"},{"location":"cli/commands/#-list-running-services","title":"<code>--list-running-services</code>","text":"<p>List currently running services with their endpoints.</p> <pre><code>python main.py --list-running-services\n</code></pre> <p>Output: <pre><code>Running Services:\n  SERVICE_ID        | JOB_ID  | NODE     | ENDPOINT\n  ollama_a1b2c3d4   | 3656789 | mel2073  | http://mel2073:11434\n  redis_e5f6g7h8    | 3656790 | mel0182  | redis://mel0182:6379\n</code></pre></p>"},{"location":"cli/commands/#sessions-automation","title":"Sessions &amp; Automation","text":""},{"location":"cli/commands/#-start-session-service_recipe-client_recipe-monitor_recipe","title":"<code>--start-session SERVICE_RECIPE CLIENT_RECIPE [MONITOR_RECIPE]</code>","text":"<p>Start a complete benchmark session with service, client, and optional monitoring.</p> <pre><code>python main.py --start-session \\\n    recipes/services/ollama_with_cadvisor.yaml \\\n    recipes/clients/ollama_benchmark.yaml \\\n    recipes/services/prometheus_with_cadvisor.yaml\n</code></pre> <p>Output: <pre><code>Starting benchmark session...\n  \u2713 Service started: ollama_abc123\n  \u2713 Waiting for service to be ready...\n  \u2713 Client started: ollama_bench_def456\n  \u2713 Prometheus started: prometheus_ghi789\n\nSession ID: session_20260115_143022\nMonitor at: http://mel0XXX:9090 (create tunnel first)\n</code></pre></p>"},{"location":"cli/commands/#-start-monitoring-service_recipe-prometheus_recipe","title":"<code>--start-monitoring SERVICE_RECIPE PROMETHEUS_RECIPE</code>","text":"<p>Start a service with monitoring (without client).</p> <pre><code>python main.py --start-monitoring \\\n    recipes/services/ollama_with_cadvisor.yaml \\\n    recipes/services/prometheus_with_cadvisor.yaml\n</code></pre>"},{"location":"cli/commands/#-stop-session-session_id","title":"<code>--stop-session SESSION_ID</code>","text":"<p>Stop all jobs in a benchmark session.</p> <pre><code>python main.py --stop-session session_20260115_143022\n</code></pre>"},{"location":"cli/commands/#monitoring-metrics","title":"Monitoring &amp; Metrics","text":""},{"location":"cli/commands/#-query-metrics-prometheus_id-query","title":"<code>--query-metrics PROMETHEUS_ID QUERY</code>","text":"<p>Query Prometheus metrics using PromQL.</p> <pre><code>python main.py --query-metrics prometheus_abc123 \"container_memory_usage_bytes\"\npython main.py --query-metrics prometheus_abc123 \"rate(container_cpu_usage_seconds_total[5m])\"\n</code></pre> <p>Output: <pre><code>Query: container_memory_usage_bytes\nResults:\n  {name=\"ollama\"}: 4294967296 (4.0 GB)\n  {name=\"redis\"}: 134217728 (128 MB)\n</code></pre></p>"},{"location":"cli/commands/#-create-tunnel-service_id-local_port-remote_port","title":"<code>--create-tunnel SERVICE_ID LOCAL_PORT REMOTE_PORT</code>","text":"<p>Generate SSH tunnel command for accessing a service.</p> <pre><code>python main.py --create-tunnel prometheus_abc123 9090 9090\n</code></pre> <p>Output: <pre><code>SSH Tunnel Command:\n  ssh -i ~/.ssh/id_ed25519_mlux -L 9090:mel0182:9090 -N u103227@login.lxp.lu -p 8822\n\nThen access: http://localhost:9090\n</code></pre></p>"},{"location":"cli/commands/#results-data","title":"Results &amp; Data","text":""},{"location":"cli/commands/#-download-results","title":"<code>--download-results</code>","text":"<p>Download benchmark results from the HPC cluster to local machine.</p> <pre><code>python main.py --download-results\n</code></pre> <p>Output: <pre><code>Downloading results from MeluXina...\n  \u2713 Downloaded: ollama_benchmark_20260115_143022.json\n  \u2713 Downloaded: redis_benchmark_20260115_150000.json\n\nResults saved to: ./results/\n</code></pre></p>"},{"location":"cli/commands/#-download-results-output-dir-path","title":"<code>--download-results --output-dir PATH</code>","text":"<p>Download results to a specific directory.</p> <pre><code>python main.py --download-results --output-dir ./my-results\n</code></pre>"},{"location":"cli/commands/#interactive-mode","title":"Interactive Mode","text":""},{"location":"cli/commands/#no-arguments","title":"No arguments","text":"<p>Run the orchestrator in interactive mode.</p> <pre><code>python main.py\n</code></pre> <p>Output: <pre><code>HPC AI Benchmarking Orchestrator\n================================\n\nCommands:\n  status      - Show running jobs\n  start       - Start a service or client\n  stop        - Stop a service\n  list        - List available services/clients\n  quit        - Exit\n\n&gt; status\n</code></pre></p>"},{"location":"cli/commands/#verbose-mode","title":"Verbose Mode","text":"<p>Add <code>--verbose</code> to any command for detailed output:</p> <pre><code>python main.py --verbose --recipe recipes/services/ollama.yaml\n</code></pre> <p>Output includes:</p> <ul> <li>SSH connection details</li> <li>Generated SLURM script content</li> <li>SLURM submission output</li> <li>Service host resolution steps</li> </ul>"},{"location":"cli/commands/#exit-codes","title":"Exit Codes","text":"Code Meaning 0 Success 1 General error 2 Configuration error 3 SSH connection failed 4 SLURM submission failed 5 Service not found"},{"location":"cli/commands/#environment-variables","title":"Environment Variables","text":"Variable Description <code>HPC_CONFIG</code> Path to config file (alternative to <code>--config</code>) <code>HPC_VERBOSE</code> Set to <code>1</code> for verbose mode <code>SSH_AUTH_SOCK</code> SSH agent socket (for key forwarding)"},{"location":"cli/commands/#examples","title":"Examples","text":"<p>See CLI Examples for common usage patterns.</p>"},{"location":"cli/examples/","title":"CLI Examples","text":"<p>Practical examples for common workflows.</p>"},{"location":"cli/examples/#basic-service-operations","title":"Basic Service Operations","text":""},{"location":"cli/examples/#start-a-service","title":"Start a Service","text":"<pre><code># Start Ollama LLM service\npython main.py --recipe recipes/services/ollama.yaml\n\n# Start with verbose output\npython main.py --verbose --recipe recipes/services/ollama.yaml\n</code></pre>"},{"location":"cli/examples/#check-status","title":"Check Status","text":"<pre><code># Simple status\npython main.py --status\n\n# Detailed status\npython main.py --verbose --status\n</code></pre>"},{"location":"cli/examples/#stop-services","title":"Stop Services","text":"<pre><code># Stop specific service\npython main.py --stop-service ollama_abc123\n\n# Stop all services\npython main.py --stop-all-services\n</code></pre>"},{"location":"cli/examples/#running-benchmarks","title":"Running Benchmarks","text":""},{"location":"cli/examples/#ollama-llm-benchmark","title":"Ollama LLM Benchmark","text":"<pre><code># 1. Start Ollama service\npython main.py --recipe recipes/services/ollama.yaml\n# Output: Service started: ollama_abc123\n\n# 2. Check service is running and get node\npython main.py --status\n# Output: ollama_abc123 | RUNNING | mel2073\n\n# 3. Run benchmark client\npython main.py --recipe recipes/clients/ollama_benchmark.yaml --target-service ollama_abc123\n\n# 4. Download results\npython main.py --download-results\n</code></pre>"},{"location":"cli/examples/#redis-benchmark","title":"Redis Benchmark","text":"<pre><code># 1. Start Redis\npython main.py --recipe recipes/services/redis.yaml\n\n# 2. Run benchmark\npython main.py --recipe recipes/clients/redis_benchmark.yaml --target-service redis_xyz789\n\n# 3. Download results\npython main.py --download-results\ncat results/redis_benchmark_*.json\n</code></pre>"},{"location":"cli/examples/#chroma-vector-db-benchmark","title":"Chroma Vector DB Benchmark","text":"<pre><code># 1. Start Chroma\npython main.py --recipe recipes/services/chroma.yaml\n\n# 2. Run benchmark\npython main.py --recipe recipes/clients/chroma_benchmark.yaml --target-service chroma_abc123\n</code></pre>"},{"location":"cli/examples/#mysql-benchmark","title":"MySQL Benchmark","text":"<pre><code># 1. Start MySQL with monitoring\npython main.py --recipe recipes/services/mysql_with_cadvisor.yaml\n\n# 2. Run benchmark\npython main.py --recipe recipes/clients/mysql_benchmark.yaml --target-service mysql_def456\n</code></pre>"},{"location":"cli/examples/#monitoring-workflows","title":"Monitoring Workflows","text":""},{"location":"cli/examples/#full-monitoring-stack","title":"Full Monitoring Stack","text":"<pre><code># Start all services with monitoring\n./scripts/start_all_services.sh\n\n# Output shows:\n#   \u2713 Ollama started on mel2073\n#   \u2713 Redis started on mel0182\n#   \u2713 Chroma started on mel0058\n#   \u2713 MySQL started on mel0222\n#   \u2713 Prometheus started on mel0210\n#   \u2713 Grafana started on mel0164\n#\n# SSH Tunnels:\n#   Prometheus: ssh -L 9090:mel0210:9090 ...\n#   Grafana:    ssh -L 3000:mel0164:3000 ...\n</code></pre>"},{"location":"cli/examples/#create-ssh-tunnels","title":"Create SSH Tunnels","text":"<pre><code># Prometheus tunnel (in one terminal)\nssh -i ~/.ssh/id_ed25519_mlux -L 9090:mel0210:9090 -N u103227@login.lxp.lu -p 8822\n\n# Grafana tunnel (in another terminal)\nssh -i ~/.ssh/id_ed25519_mlux -L 3000:mel0164:3000 -N u103227@login.lxp.lu -p 8822\n\n# Then open in browser:\n#   http://localhost:9090 - Prometheus\n#   http://localhost:3000 - Grafana (admin/admin)\n</code></pre>"},{"location":"cli/examples/#query-metrics","title":"Query Metrics","text":"<pre><code># Get memory usage\npython main.py --query-metrics prometheus_abc123 \"container_memory_usage_bytes\"\n\n# Get CPU usage rate\npython main.py --query-metrics prometheus_abc123 \"rate(container_cpu_usage_seconds_total[5m])\"\n\n# Get network traffic\npython main.py --query-metrics prometheus_abc123 \"rate(container_network_receive_bytes_total[1m])\"\n</code></pre>"},{"location":"cli/examples/#automated-sessions","title":"Automated Sessions","text":""},{"location":"cli/examples/#complete-benchmark-session","title":"Complete Benchmark Session","text":"<pre><code># Start service + client + monitoring in one command\npython main.py --start-session \\\n    recipes/services/ollama_with_cadvisor.yaml \\\n    recipes/clients/ollama_benchmark.yaml \\\n    recipes/services/prometheus_with_cadvisor.yaml\n</code></pre>"},{"location":"cli/examples/#service-monitoring-only","title":"Service + Monitoring Only","text":"<pre><code># Start service with monitoring (no client)\npython main.py --start-monitoring \\\n    recipes/services/redis_with_cadvisor.yaml \\\n    recipes/services/prometheus_with_cadvisor.yaml\n</code></pre>"},{"location":"cli/examples/#parametric-benchmarks","title":"Parametric Benchmarks","text":""},{"location":"cli/examples/#redis-parametric-sweep","title":"Redis Parametric Sweep","text":"<pre><code># Run comprehensive Redis benchmark across multiple configurations\n./scripts/run_redis_parametric.sh\n\n# This tests:\n#   - Client counts: 1, 10, 50, 100\n#   - Data sizes: 64B, 256B, 1KB, 4KB\n#   - Pipeline depths: 1, 10, 50\n</code></pre>"},{"location":"cli/examples/#chroma-parametric-sweep","title":"Chroma Parametric Sweep","text":"<pre><code>./scripts/run_chroma_parametric.sh\n</code></pre>"},{"location":"cli/examples/#ollama-parametric-sweep","title":"Ollama Parametric Sweep","text":"<pre><code>./scripts/run_ollama_parametric.sh\n</code></pre>"},{"location":"cli/examples/#results-management","title":"Results Management","text":""},{"location":"cli/examples/#download-all-results","title":"Download All Results","text":"<pre><code># Download to default ./results/ directory\npython main.py --download-results\n\n# Download to custom directory\npython main.py --download-results --output-dir ./benchmark-results-2026\n</code></pre>"},{"location":"cli/examples/#view-results","title":"View Results","text":"<pre><code># List downloaded results\nls -la results/\n\n# View JSON results\ncat results/ollama_benchmark_20260115_143022.json | jq .\n\n# View summary\ncat results/ollama_benchmark_20260115_143022.json | jq '.summary'\n</code></pre>"},{"location":"cli/examples/#debugging","title":"Debugging","text":""},{"location":"cli/examples/#verbose-mode","title":"Verbose Mode","text":"<pre><code># See full SSH and SLURM interaction\npython main.py --verbose --recipe recipes/services/ollama.yaml\n</code></pre>"},{"location":"cli/examples/#view-generated-script","title":"View Generated Script","text":"<pre><code># Scripts are saved in scripts/ directory\nls scripts/\ncat scripts/service_ollama_*.sh\n</code></pre>"},{"location":"cli/examples/#check-slurm-logs","title":"Check SLURM Logs","text":"<pre><code># SSH to MeluXina\nssh -p 8822 u103227@login.lxp.lu\n\n# View job output\ncat slurm-3656789.out\ncat slurm-3656789.err\n</code></pre>"},{"location":"cli/examples/#multiple-services","title":"Multiple Services","text":""},{"location":"cli/examples/#start-multiple-services","title":"Start Multiple Services","text":"<pre><code># Start services in sequence\npython main.py --recipe recipes/services/ollama.yaml\npython main.py --recipe recipes/services/redis.yaml\npython main.py --recipe recipes/services/chroma.yaml\n\n# Or use automation script\n./scripts/start_all_services.sh\n</code></pre>"},{"location":"cli/examples/#run-multiple-clients","title":"Run Multiple Clients","text":"<pre><code># After services are running\n./scripts/start_all_clients.sh\n\n# Or manually\npython main.py --recipe recipes/clients/ollama_benchmark.yaml --target-service ollama_xxx\npython main.py --recipe recipes/clients/redis_benchmark.yaml --target-service redis_xxx\n</code></pre>"},{"location":"cli/examples/#error-recovery","title":"Error Recovery","text":""},{"location":"cli/examples/#service-failed-to-start","title":"Service Failed to Start","text":"<pre><code># Check status for error\npython main.py --status\n\n# View SLURM logs\nssh -p 8822 u103227@login.lxp.lu \"cat slurm-*.err\"\n\n# Try again with verbose\npython main.py --verbose --recipe recipes/services/ollama.yaml\n</code></pre>"},{"location":"cli/examples/#stop-stuck-jobs","title":"Stop Stuck Jobs","text":"<pre><code># Stop all services\npython main.py --stop-all-services\n\n# Or directly via SLURM\nssh -p 8822 u103227@login.lxp.lu \"scancel -u \\$USER\"\n</code></pre>"},{"location":"cli/examples/#clean-up","title":"Clean Up","text":"<pre><code># Stop everything\npython main.py --stop-all-services\n\n# Remove generated scripts\nrm -f scripts/service_*.sh scripts/client_*.sh\n\n# Clear local service ID cache\nrm -f .service_ids\n</code></pre>"},{"location":"cli/examples/#tips-best-practices","title":"Tips &amp; Best Practices","text":"<p>Always Check Status</p> <p>Run <code>python main.py --status</code> before starting new services to avoid resource conflicts.</p> <p>Use Verbose Mode for Debugging</p> <p>Add <code>--verbose</code> when something isn't working to see detailed logs.</p> <p>Create Tunnels in Background</p> <p>Use <code>-f</code> flag for SSH tunnels: <code>ssh -f -N -L 3000:mel0164:3000 ...</code></p> <p>Resource Limits</p> <p>Be mindful of your SLURM allocation limits. Stop services when not in use.</p>"},{"location":"development/extending/","title":"Extending the Framework","text":"<p>Guide for adding new features and customizations to the orchestrator.</p>"},{"location":"development/extending/#architecture-recap","title":"Architecture Recap","text":"<pre><code>graph TB\n    subgraph Core[\"Core Framework\"]\n        Orch[\"BenchmarkOrchestrator\"]\n        Servers[\"ServersModule\"]\n        Clients[\"ClientsModule\"]\n        Factory[\"JobFactory\"]\n    end\n\n    subgraph Services[\"Service Classes\"]\n        Base[\"BaseJob\"]\n        Service[\"Service\"]\n        Client[\"Client\"]\n    end\n\n    subgraph Implementations[\"Implementations\"]\n        Ollama[\"OllamaService\"]\n        Redis[\"RedisService\"]\n        NewSvc[\"NewService\"]\n    end\n\n    Orch --&gt; Servers\n    Orch --&gt; Clients\n    Servers --&gt; Factory\n    Clients --&gt; Factory\n    Factory --&gt; Base\n    Base --&gt; Service\n    Base --&gt; Client\n    Service --&gt; Ollama\n    Service --&gt; Redis\n    Service --&gt; NewSvc\n\n    style NewSvc fill:#FFE0B2</code></pre>"},{"location":"development/extending/#adding-a-new-service","title":"Adding a New Service","text":""},{"location":"development/extending/#step-1-create-service-class","title":"Step 1: Create Service Class","text":"<pre><code># src/services/new_service.py\nfrom typing import List\nfrom .base import Service\nfrom ..base import JobFactory\n\nclass NewService(Service):\n    \"\"\"New service implementation\"\"\"\n\n    def __init__(self, config: dict):\n        super().__init__(config)\n        # Extract service-specific config\n        self.custom_option = config.get('custom_option', 'default')\n        self.port = config.get('ports', [8080])[0]\n\n    def get_service_setup_commands(self) -&gt; List[str]:\n        \"\"\"Setup commands run before container starts\"\"\"\n        commands = super().get_service_setup_commands()\n        commands.extend([\n            \"\",\n            \"# NewService setup\",\n            f\"export NEW_SERVICE_PORT={self.port}\",\n            f\"export CUSTOM_OPTION={self.custom_option}\",\n            \"mkdir -p $HOME/new_service/data\",\n            \"mkdir -p $HOME/new_service/logs\",\n        ])\n        return commands\n\n    def get_container_command(self) -&gt; str:\n        \"\"\"Container execution command\"\"\"\n        container_path = self._resolve_container_path()\n\n        cmd_parts = [\n            \"apptainer exec\",\n            \"--bind $HOME/new_service/data:/data\",\n            \"--bind $HOME/new_service/logs:/logs\",\n        ]\n\n        # Add GPU support if needed\n        if self.resources.get('gres'):\n            cmd_parts.insert(1, \"--nv\")\n\n        cmd_parts.extend([\n            container_path,\n            f\"new_service_binary --port {self.port}\",\n            \"&amp;\"  # Run in background\n        ])\n\n        return \" \\\\\\n    \".join(cmd_parts)\n\n    def get_health_check_commands(self) -&gt; List[str]:\n        \"\"\"Health check after container starts\"\"\"\n        return [\n            \"\",\n            \"# Wait for NewService to start\",\n            \"sleep 10\",\n            f\"curl -s http://localhost:{self.port}/health || echo 'Health check pending'\",\n            \"\",\n            f\"echo 'NewService running on port {self.port}'\",\n        ]\n\n\n# Register with JobFactory\nJobFactory.register_service('new_service', NewService)\n</code></pre>"},{"location":"development/extending/#step-2-register-in-__init__py","title":"Step 2: Register in <code>__init__.py</code>","text":"<pre><code># src/services/__init__.py\nfrom .new_service import NewService\n\n__all__ = [\n    # ... existing services\n    'NewService',\n]\n</code></pre>"},{"location":"development/extending/#step-3-create-recipe","title":"Step 3: Create Recipe","text":"<pre><code># recipes/services/new_service.yaml\nservice:\n  name: new_service\n  description: \"My new service\"\n\n  container:\n    docker_source: docker://myimage:latest\n    image_path: $HOME/containers/new_service.sif\n\n  custom_option: \"production\"\n\n  resources:\n    cpus_per_task: 4\n    mem: \"8G\"\n    time: \"02:00:00\"\n    partition: cpu\n\n  ports:\n    - 8080\n</code></pre>"},{"location":"development/extending/#step-4-test","title":"Step 4: Test","text":"<pre><code>python main.py --verbose --recipe recipes/services/new_service.yaml\n</code></pre>"},{"location":"development/extending/#adding-a-new-client","title":"Adding a New Client","text":""},{"location":"development/extending/#step-1-create-client-class","title":"Step 1: Create Client Class","text":"<pre><code># src/services/new_client.py\nfrom typing import List\nfrom .base import Client\nfrom ..base import JobFactory\n\nclass NewBenchmarkClient(Client):\n    \"\"\"Benchmark client for NewService\"\"\"\n\n    def __init__(self, config: dict):\n        super().__init__(config)\n        params = config.get('parameters', {})\n        self.num_requests = params.get('num_requests', 1000)\n        self.concurrent = params.get('concurrent', 10)\n        self.output_file = params.get('output_file', '$HOME/results/new_benchmark.json')\n\n    def get_benchmark_commands(self) -&gt; List[str]:\n        \"\"\"Commands to run the benchmark\"\"\"\n        return [\n            \"\",\n            \"# NewService Benchmark\",\n            f\"echo 'Running benchmark against {self.target_endpoint}'\",\n            \"\",\n            f\"python3 benchmark_scripts/new_benchmark.py \\\\\",\n            f\"    --endpoint {self.target_endpoint} \\\\\",\n            f\"    --requests {self.num_requests} \\\\\",\n            f\"    --concurrent {self.concurrent} \\\\\",\n            f\"    --output {self.output_file}\",\n            \"\",\n            f\"echo 'Results saved to {self.output_file}'\",\n        ]\n\n\n# Register with JobFactory\nJobFactory.register_client('new_benchmark', NewBenchmarkClient)\n</code></pre>"},{"location":"development/extending/#step-2-create-benchmark-script","title":"Step 2: Create Benchmark Script","text":"<pre><code># benchmark_scripts/new_benchmark.py\nimport argparse\nimport json\nimport time\nimport requests\nfrom concurrent.futures import ThreadPoolExecutor\n\ndef run_request(endpoint):\n    start = time.time()\n    try:\n        response = requests.get(f\"{endpoint}/api/test\")\n        return {\n            'success': response.status_code == 200,\n            'latency': time.time() - start,\n            'status': response.status_code\n        }\n    except Exception as e:\n        return {'success': False, 'latency': time.time() - start, 'error': str(e)}\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--endpoint', required=True)\n    parser.add_argument('--requests', type=int, default=1000)\n    parser.add_argument('--concurrent', type=int, default=10)\n    parser.add_argument('--output', required=True)\n    args = parser.parse_args()\n\n    results = []\n    with ThreadPoolExecutor(max_workers=args.concurrent) as executor:\n        futures = [executor.submit(run_request, args.endpoint) \n                   for _ in range(args.requests)]\n        results = [f.result() for f in futures]\n\n    # Calculate statistics\n    latencies = [r['latency'] for r in results if r['success']]\n    output = {\n        'endpoint': args.endpoint,\n        'total_requests': args.requests,\n        'successful': sum(1 for r in results if r['success']),\n        'failed': sum(1 for r in results if not r['success']),\n        'latency_avg': sum(latencies) / len(latencies) if latencies else 0,\n        'latency_p99': sorted(latencies)[int(len(latencies) * 0.99)] if latencies else 0,\n    }\n\n    with open(args.output, 'w') as f:\n        json.dump(output, f, indent=2)\n\n    print(json.dumps(output, indent=2))\n\nif __name__ == '__main__':\n    main()\n</code></pre>"},{"location":"development/extending/#step-3-create-client-recipe","title":"Step 3: Create Client Recipe","text":"<pre><code># recipes/clients/new_benchmark.yaml\nclient:\n  name: new_benchmark\n  type: new_benchmark\n\n  parameters:\n    num_requests: 1000\n    concurrent: 10\n    output_file: \"$HOME/results/new_benchmark.json\"\n\n  resources:\n    cpus_per_task: 2\n    mem: \"4G\"\n    time: \"00:30:00\"\n    partition: cpu\n</code></pre>"},{"location":"development/extending/#customizing-slurm-script-generation","title":"Customizing SLURM Script Generation","text":"<p>Override <code>generate_slurm_script()</code> for full control:</p> <pre><code>class CustomService(Service):\n    def generate_slurm_script(self) -&gt; str:\n        \"\"\"Fully custom SLURM script\"\"\"\n        script = [\n            \"#!/bin/bash\",\n            f\"#SBATCH --job-name={self.job_id}\",\n            f\"#SBATCH --account={self.account}\",\n            # ... more SBATCH directives\n            \"\",\n            \"# Custom script content\",\n            \"module load MyModule\",\n            \"\",\n            \"# Run service\",\n            \"my_custom_command\",\n        ]\n        return '\\n'.join(script)\n</code></pre>"},{"location":"development/extending/#testing-new-components","title":"Testing New Components","text":"<pre><code># Test service class instantiation\npython -c \"\nfrom src.services.new_service import NewService\nconfig = {'name': 'test', 'container': {'image_path': 'test.sif'}}\nsvc = NewService(config)\nprint(svc.get_setup_commands())\n\"\n\n# Test recipe loading\npython main.py --verbose --recipe recipes/services/new_service.yaml\n\n# Check generated script\ncat scripts/service_new_service_*.sh\n</code></pre> <p>Next: Adding New Services</p>"},{"location":"development/new-services/","title":"Adding New Services","text":"<p>Step-by-step guide for integrating a new service into the orchestrator.</p>"},{"location":"development/new-services/#prerequisites","title":"Prerequisites","text":"<p>Before adding a new service, ensure you have:</p> <ol> <li>A Docker image for the service</li> <li>Understanding of the service's configuration</li> <li>Knowledge of exposed ports and protocols</li> </ol>"},{"location":"development/new-services/#step-by-step-guide","title":"Step-by-Step Guide","text":""},{"location":"development/new-services/#step-1-prepare-container-image","title":"Step 1: Prepare Container Image","text":"<pre><code># On MeluXina\nmodule load Apptainer/1.2.4-GCCcore-12.3.0\ncd $HOME/containers\n\n# Pull from Docker Hub\napptainer pull my_service.sif docker://myorg/myservice:latest\n\n# Or build from definition file\napptainer build my_service.sif my_service.def\n</code></pre>"},{"location":"development/new-services/#step-2-create-service-class","title":"Step 2: Create Service Class","text":"<p>Create <code>src/services/my_service.py</code>:</p> <pre><code>\"\"\"\nMyService - Description of what this service does\n\"\"\"\nfrom typing import List\nfrom .base import Service\nfrom ..base import JobFactory\n\n\nclass MyService(Service):\n    \"\"\"\n    MyService implementation for HPC deployment.\n\n    Attributes:\n        port: Service port (default: 8080)\n        config_option: Custom configuration option\n    \"\"\"\n\n    def __init__(self, config: dict):\n        super().__init__(config)\n\n        # Extract configuration\n        service_config = config.get('service', config)\n        self.port = service_config.get('ports', [8080])[0]\n        self.config_option = service_config.get('config_option', 'default')\n\n        # Data directories\n        self.data_dir = f\"$HOME/my_service/{self.job_id}/data\"\n        self.log_dir = f\"$HOME/my_service/{self.job_id}/logs\"\n\n    def get_service_setup_commands(self) -&gt; List[str]:\n        \"\"\"\n        Setup commands executed before container starts.\n\n        Returns:\n            List of bash commands for setup\n        \"\"\"\n        commands = super().get_service_setup_commands()\n        commands.extend([\n            \"\",\n            \"# MyService setup\",\n            f\"export MY_SERVICE_PORT={self.port}\",\n            f\"export MY_SERVICE_CONFIG={self.config_option}\",\n            \"\",\n            \"# Create directories\",\n            f\"mkdir -p {self.data_dir}\",\n            f\"mkdir -p {self.log_dir}\",\n            \"\",\n            \"# Download any required files\",\n            \"# curl -o $HOME/my_service/config.yaml https://example.com/config.yaml\",\n        ])\n        return commands\n\n    def get_container_command(self) -&gt; str:\n        \"\"\"\n        Generate Apptainer execution command.\n\n        Returns:\n            Apptainer exec command string\n        \"\"\"\n        container_path = self._resolve_container_path()\n\n        cmd_parts = [\"apptainer exec\"]\n\n        # GPU support\n        if self._needs_gpu():\n            cmd_parts.append(\"--nv\")\n\n        # Bind mounts\n        cmd_parts.extend([\n            f\"--bind {self.data_dir}:/data\",\n            f\"--bind {self.log_dir}:/logs\",\n        ])\n\n        # Environment variables\n        for key, value in self.environment.items():\n            cmd_parts.append(f\"--env {key}={value}\")\n\n        # Container and command\n        cmd_parts.extend([\n            container_path,\n            f\"my_service_binary --port {self.port} --config /data/config.yaml\",\n            \"&amp;\"  # Background\n        ])\n\n        return \" \\\\\\n    \".join(cmd_parts)\n\n    def get_health_check_commands(self) -&gt; List[str]:\n        \"\"\"\n        Health check commands to verify service is running.\n\n        Returns:\n            List of bash commands for health checking\n        \"\"\"\n        return [\n            \"\",\n            \"# Wait for MyService to initialize\",\n            \"sleep 15\",\n            \"\",\n            \"# Health check loop\",\n            \"for i in {1..10}; do\",\n            f\"    if curl -s http://localhost:{self.port}/health | grep -q 'ok'; then\",\n            \"        echo 'MyService is healthy!'\",\n            \"        break\",\n            \"    fi\",\n            \"    echo \\\"Waiting for MyService... ($i/10)\\\"\",\n            \"    sleep 5\",\n            \"done\",\n            \"\",\n            \"# Display endpoint\",\n            \"echo '========================================='\",\n            \"echo 'MyService is running on:'\",\n            f\"echo \\\"http://$(hostname):{self.port}\\\"\",\n            \"echo '========================================='\",\n            \"\",\n            \"# Keep process alive\",\n            \"wait\",\n        ]\n\n    def _needs_gpu(self) -&gt; bool:\n        \"\"\"Check if service needs GPU\"\"\"\n        return 'gpu' in str(self.resources.get('gres', '')).lower()\n\n\n# Register with factory\nJobFactory.register_service('my_service', MyService)\n</code></pre>"},{"location":"development/new-services/#step-3-register-service","title":"Step 3: Register Service","text":"<p>Add to <code>src/services/__init__.py</code>:</p> <pre><code>from .my_service import MyService\n\n__all__ = [\n    'OllamaService',\n    'RedisService',\n    # ... other services\n    'MyService',  # Add this\n]\n</code></pre>"},{"location":"development/new-services/#step-4-create-service-recipe","title":"Step 4: Create Service Recipe","text":"<p>Create <code>recipes/services/my_service.yaml</code>:</p> <pre><code>service:\n  name: my_service\n  description: \"MyService - What it does\"\n\n  container:\n    docker_source: docker://myorg/myservice:latest\n    image_path: $HOME/containers/my_service.sif\n\n  # Service-specific options\n  config_option: \"production\"\n\n  # SLURM resources\n  resources:\n    nodes: 1\n    ntasks: 1\n    cpus_per_task: 4\n    mem: \"8G\"\n    time: \"02:00:00\"\n    partition: cpu\n    qos: default\n\n  # Environment variables\n  environment:\n    MY_SERVICE_LOG_LEVEL: \"info\"\n    MY_SERVICE_MAX_CONNECTIONS: \"100\"\n\n  # Exposed ports\n  ports:\n    - 8080\n</code></pre>"},{"location":"development/new-services/#step-5-create-monitoring-recipe-optional","title":"Step 5: Create Monitoring Recipe (Optional)","text":"<p>Create <code>recipes/services/my_service_with_cadvisor.yaml</code>:</p> <pre><code>service:\n  name: my_service\n  description: \"MyService with monitoring\"\n\n  container:\n    docker_source: docker://myorg/myservice:latest\n    image_path: $HOME/containers/my_service.sif\n\n  config_option: \"production\"\n\n  resources:\n    cpus_per_task: 4\n    mem: \"8G\"\n    time: \"02:00:00\"\n    partition: cpu\n\n  environment:\n    MY_SERVICE_LOG_LEVEL: \"info\"\n\n  ports:\n    - 8080\n\n  # Enable monitoring\n  enable_cadvisor: true\n  cadvisor_port: 8081\n</code></pre>"},{"location":"development/new-services/#step-6-test-the-service","title":"Step 6: Test the Service","text":"<pre><code># Verbose mode to see generated script\npython main.py --verbose --recipe recipes/services/my_service.yaml\n\n# Check status\npython main.py --status\n\n# View generated script\ncat scripts/service_my_service_*.sh\n\n# Stop service\npython main.py --stop-service my_service_xxx\n</code></pre>"},{"location":"development/new-services/#adding-a-benchmark-client","title":"Adding a Benchmark Client","text":""},{"location":"development/new-services/#step-1-create-client-class","title":"Step 1: Create Client Class","text":"<p>Create <code>src/services/my_service.py</code> (add to existing file):</p> <pre><code>class MyServiceBenchmarkClient(Client):\n    \"\"\"Benchmark client for MyService\"\"\"\n\n    def __init__(self, config: dict):\n        super().__init__(config)\n        params = config.get('parameters', {})\n        self.requests = params.get('requests', 1000)\n        self.concurrent = params.get('concurrent', 10)\n        self.duration = params.get('duration', 60)\n        self.output_file = params.get('output_file', '$HOME/results/my_service_bench.json')\n\n    def get_benchmark_commands(self) -&gt; List[str]:\n        return [\n            \"\",\n            \"# MyService Benchmark\",\n            f\"echo 'Benchmarking {self.target_endpoint}'\",\n            f\"echo 'Requests: {self.requests}, Concurrent: {self.concurrent}'\",\n            \"\",\n            f\"python3 benchmark_scripts/my_service_benchmark.py \\\\\",\n            f\"    --endpoint {self.target_endpoint} \\\\\",\n            f\"    --requests {self.requests} \\\\\",\n            f\"    --concurrent {self.concurrent} \\\\\",\n            f\"    --duration {self.duration} \\\\\",\n            f\"    --output {self.output_file}\",\n            \"\",\n            f\"echo 'Benchmark complete. Results: {self.output_file}'\",\n        ]\n\n\n# Register client\nJobFactory.register_client('my_service_benchmark', MyServiceBenchmarkClient)\n</code></pre>"},{"location":"development/new-services/#step-2-create-client-recipe","title":"Step 2: Create Client Recipe","text":"<p>Create <code>recipes/clients/my_service_benchmark.yaml</code>:</p> <pre><code>client:\n  name: my_service_benchmark\n  type: my_service_benchmark\n  description: \"Performance benchmark for MyService\"\n\n  parameters:\n    requests: 10000\n    concurrent: 50\n    duration: 120\n    output_file: \"$HOME/results/my_service_benchmark.json\"\n\n  resources:\n    cpus_per_task: 4\n    mem: \"4G\"\n    time: \"00:30:00\"\n    partition: cpu\n</code></pre>"},{"location":"development/new-services/#step-3-create-benchmark-script","title":"Step 3: Create Benchmark Script","text":"<p>Create <code>benchmark_scripts/my_service_benchmark.py</code>:</p> <pre><code>#!/usr/bin/env python3\n\"\"\"MyService benchmark implementation\"\"\"\nimport argparse\nimport json\nimport time\nimport statistics\nfrom concurrent.futures import ThreadPoolExecutor\nimport requests\n\ndef benchmark_request(endpoint):\n    \"\"\"Single benchmark request\"\"\"\n    start = time.time()\n    try:\n        resp = requests.get(f\"{endpoint}/api/test\", timeout=30)\n        return {\n            'success': resp.status_code == 200,\n            'latency': time.time() - start,\n            'status': resp.status_code\n        }\n    except Exception as e:\n        return {\n            'success': False,\n            'latency': time.time() - start,\n            'error': str(e)\n        }\n\ndef main():\n    parser = argparse.ArgumentParser(description='MyService Benchmark')\n    parser.add_argument('--endpoint', required=True)\n    parser.add_argument('--requests', type=int, default=1000)\n    parser.add_argument('--concurrent', type=int, default=10)\n    parser.add_argument('--duration', type=int, default=60)\n    parser.add_argument('--output', required=True)\n    args = parser.parse_args()\n\n    print(f\"Starting benchmark against {args.endpoint}\")\n    print(f\"Requests: {args.requests}, Concurrent: {args.concurrent}\")\n\n    start_time = time.time()\n    results = []\n\n    with ThreadPoolExecutor(max_workers=args.concurrent) as executor:\n        futures = [\n            executor.submit(benchmark_request, args.endpoint)\n            for _ in range(args.requests)\n        ]\n        results = [f.result() for f in futures]\n\n    total_time = time.time() - start_time\n\n    # Calculate statistics\n    successful = [r for r in results if r['success']]\n    latencies = [r['latency'] for r in successful]\n\n    output = {\n        'benchmark': 'my_service_benchmark',\n        'endpoint': args.endpoint,\n        'config': {\n            'requests': args.requests,\n            'concurrent': args.concurrent\n        },\n        'results': {\n            'total_requests': len(results),\n            'successful': len(successful),\n            'failed': len(results) - len(successful),\n            'success_rate': len(successful) / len(results) * 100,\n            'total_duration': total_time,\n            'requests_per_second': len(results) / total_time,\n        },\n        'latency': {\n            'mean': statistics.mean(latencies) if latencies else 0,\n            'median': statistics.median(latencies) if latencies else 0,\n            'stdev': statistics.stdev(latencies) if len(latencies) &gt; 1 else 0,\n            'min': min(latencies) if latencies else 0,\n            'max': max(latencies) if latencies else 0,\n            'p95': sorted(latencies)[int(len(latencies) * 0.95)] if latencies else 0,\n            'p99': sorted(latencies)[int(len(latencies) * 0.99)] if latencies else 0,\n        }\n    }\n\n    # Save results\n    with open(args.output, 'w') as f:\n        json.dump(output, f, indent=2)\n\n    # Print summary\n    print(\"\\n\" + \"=\"*50)\n    print(\"BENCHMARK RESULTS\")\n    print(\"=\"*50)\n    print(f\"Success Rate: {output['results']['success_rate']:.2f}%\")\n    print(f\"Throughput: {output['results']['requests_per_second']:.2f} req/s\")\n    print(f\"Latency (mean): {output['latency']['mean']*1000:.2f} ms\")\n    print(f\"Latency (p99): {output['latency']['p99']*1000:.2f} ms\")\n    print(f\"Results saved to: {args.output}\")\n\nif __name__ == '__main__':\n    main()\n</code></pre>"},{"location":"development/new-services/#step-4-test-client","title":"Step 4: Test Client","text":"<pre><code># Start service\npython main.py --recipe recipes/services/my_service.yaml\n\n# Run benchmark\npython main.py --recipe recipes/clients/my_service_benchmark.yaml --target-service my_service_xxx\n\n# Download results\npython main.py --download-results\n</code></pre>"},{"location":"development/new-services/#checklist","title":"Checklist","text":"<ul> <li> Container image available on MeluXina</li> <li> Service class created in <code>src/services/</code></li> <li> Service registered in <code>__init__.py</code></li> <li> Service recipe created</li> <li> Service recipe with cAdvisor created (optional)</li> <li> Client class created (optional)</li> <li> Benchmark script created (optional)</li> <li> Client recipe created (optional)</li> <li> Documentation updated</li> </ul> <p>See also: Extending the Framework | Recipes Overview</p>"},{"location":"getting-started/installation/","title":"Installation Guide","text":"<p>This page provides detailed installation instructions for the HPC AI Benchmarking Orchestrator.</p>"},{"location":"getting-started/installation/#prerequisites","title":"Prerequisites","text":""},{"location":"getting-started/installation/#requirements","title":"Requirements","text":"<ul> <li>Python 3.9+ installed locally</li> <li>SSH access to MeluXina supercomputer</li> <li>SLURM allocation (account: <code>p200981</code> or your project account)</li> <li>SSH key configured for MeluXina</li> <li>Git for cloning the repository</li> </ul>"},{"location":"getting-started/installation/#installation-steps","title":"Installation Steps","text":""},{"location":"getting-started/installation/#1-clone-the-repository","title":"1. Clone the Repository","text":"<pre><code>cd $HOME\ngit clone https://github.com/ChrisKarg/team10_EUMASTER4HPC2526_challenge.git\ncd team10_EUMASTER4HPC2526_challenge\n</code></pre>"},{"location":"getting-started/installation/#2-create-virtual-environment","title":"2. Create Virtual Environment","text":"<pre><code># Create virtual environment\npython -m venv .venv\n\n# Activate it\nsource .venv/bin/activate  # Linux/macOS\n# or\n.venv\\Scripts\\activate     # Windows\n\n# Install dependencies\npip install -r requirements.txt\n</code></pre>"},{"location":"getting-started/installation/#3-configure-hpc-connection","title":"3. Configure HPC Connection","text":"<p>Create or edit <code>config.yaml</code> in the project root:</p> <pre><code># SSH Configuration for MeluXina\nssh:\n  hostname: \"login.lxp.lu\"\n  port: 8822\n  username: \"u103227\"  # Your MeluXina username\n  key_filename: \"~/.ssh/id_ed25519_mlux\"  # Path to your SSH key\n\n# SLURM Configuration\nslurm:\n  account: \"p200981\"    # Your project account\n  partition: \"gpu\"      # Default partition (gpu, cpu, fpga)\n  qos: \"default\"        # Quality of Service\n  time: \"02:00:00\"      # Default time limit\n\n# Container Configuration\ncontainers:\n  base_path: \"$HOME/containers\"\n</code></pre> <p>SSH Key Setup</p> <p>Make sure your SSH key is added to your MeluXina account. Test with: <pre><code>ssh -i ~/.ssh/id_ed25519_mlux -p 8822 u103227@login.lxp.lu\n</code></pre></p>"},{"location":"getting-started/installation/#4-verify-installation","title":"4. Verify Installation","text":"<p>Test the installation by checking available commands:</p> <pre><code>python main.py --help\n</code></pre> <p>Expected output:</p> <pre><code>usage: main.py [-h] [--recipe RECIPE] [--status] [--list-services]\n               [--list-clients] [--stop-service STOP_SERVICE]\n               [--stop-all-services] ...\n\nHPC AI Benchmarking Orchestrator\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --recipe RECIPE       Path to YAML recipe file\n  --status              Show status of all running jobs\n  --list-services       List available service types\n  --list-clients        List available client types\n  ...\n</code></pre>"},{"location":"getting-started/installation/#5-test-ssh-connection","title":"5. Test SSH Connection","text":"<p>Verify the orchestrator can connect to MeluXina:</p> <pre><code>python main.py --status\n</code></pre> <p>Expected output (if no jobs running):</p> <pre><code>SLURM Job Status:\n  Total Jobs: 0\n  Services: 0\n  Clients: 0\n</code></pre>"},{"location":"getting-started/installation/#container-images","title":"Container Images","text":"<p>The orchestrator uses Apptainer (Singularity) containers. Container images are automatically pulled on first use, but you can pre-build them:</p>"},{"location":"getting-started/installation/#on-meluxina-via-ssh","title":"On MeluXina (via SSH)","text":"<pre><code># SSH to MeluXina\nssh -p 8822 u103227@login.lxp.lu\n\n# Load Apptainer module\nmodule load Apptainer/1.2.4-GCCcore-12.3.0\n\n# Create containers directory\nmkdir -p $HOME/containers\ncd $HOME/containers\n\n# Pull required images\napptainer pull ollama_latest.sif docker://ollama/ollama:latest\napptainer pull redis_latest.sif docker://redis:latest\napptainer pull chroma_latest.sif docker://chromadb/chroma:latest\napptainer pull mysql_latest.sif docker://mysql:8.0\napptainer pull prometheus.sif docker://prom/prometheus:latest\napptainer pull grafana.sif docker://grafana/grafana:latest\napptainer pull cadvisor.sif docker://gcr.io/cadvisor/cadvisor:latest\n</code></pre>"},{"location":"getting-started/installation/#directory-structure","title":"Directory Structure","text":"<p>After installation, your project should look like:</p> <pre><code>team10_EUMASTER4HPC2526_challenge/\n\u251c\u2500\u2500 main.py                 # CLI entry point\n\u251c\u2500\u2500 config.yaml             # Your configuration\n\u251c\u2500\u2500 requirements.txt        # Python dependencies\n\u251c\u2500\u2500 src/                    # Core modules\n\u2502   \u251c\u2500\u2500 orchestrator.py\n\u2502   \u251c\u2500\u2500 servers.py\n\u2502   \u251c\u2500\u2500 clients.py\n\u2502   \u2514\u2500\u2500 services/           # Service implementations\n\u251c\u2500\u2500 recipes/                # YAML recipes\n\u2502   \u251c\u2500\u2500 services/           # Service definitions\n\u2502   \u2514\u2500\u2500 clients/            # Client definitions\n\u251c\u2500\u2500 benchmark_scripts/      # Benchmark implementations\n\u251c\u2500\u2500 scripts/                # Automation scripts\n\u2514\u2500\u2500 docs-src/               # Documentation\n</code></pre>"},{"location":"getting-started/installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/installation/#ssh-connection-issues","title":"SSH Connection Issues","text":"<pre><code># Test SSH directly\nssh -v -i ~/.ssh/id_ed25519_mlux -p 8822 u103227@login.lxp.lu\n\n# Check key permissions\nchmod 600 ~/.ssh/id_ed25519_mlux\n</code></pre>"},{"location":"getting-started/installation/#slurm-account-issues","title":"SLURM Account Issues","text":"<pre><code># On MeluXina, check your accounts\nsacctmgr show associations user=$USER\n\n# Verify account in config.yaml matches\n</code></pre>"},{"location":"getting-started/installation/#container-not-found","title":"Container Not Found","text":"<pre><code># On MeluXina, verify container exists\nls -la $HOME/containers/\n\n# Check container path in recipe matches\n</code></pre>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<ul> <li>Quick Start - Run your first benchmark</li> <li>CLI Reference - All available commands</li> <li>Services - Configure different services</li> </ul> <p>Continue to Quick Start \u2192</p>"},{"location":"getting-started/overview/","title":"Getting Started","text":"<p>This guide will help you set up and run the HPC AI Benchmarking Orchestrator on the MeluXina supercomputer.</p>"},{"location":"getting-started/overview/#overview","title":"Overview","text":"<p>The HPC AI Benchmarking Orchestrator is a Python framework for deploying, benchmarking, and monitoring containerized AI services on HPC clusters. It automates the complex workflow of SLURM job submission, container management, and metrics collection.</p> <p>Key Capabilities:</p> <ul> <li>Deploy AI services (Ollama, Redis, Chroma, MySQL) via SLURM</li> <li>Run automated benchmark workloads with configurable parameters</li> <li>Collect real-time metrics via Prometheus and cAdvisor</li> <li>Visualize performance through Grafana dashboards</li> <li>Generate benchmark reports and analysis</li> </ul>"},{"location":"getting-started/overview/#system-requirements","title":"System Requirements","text":""},{"location":"getting-started/overview/#local-machine","title":"Local Machine","text":"<ul> <li>Python 3.9+</li> <li>SSH client with key-based authentication</li> <li>Git</li> </ul>"},{"location":"getting-started/overview/#hpc-cluster-meluxina","title":"HPC Cluster (MeluXina)","text":"<ul> <li>SLURM workload manager</li> <li>Apptainer/Singularity for containerization</li> <li>Access to GPU nodes (for Ollama)</li> <li>Project allocation (account: <code>p200981</code> or your project)</li> </ul>"},{"location":"getting-started/overview/#quick-overview","title":"Quick Overview","text":"<pre><code>sequenceDiagram\n    participant User\n    participant CLI as main.py\n    participant SSH as SSHClient\n    participant SLURM\n    participant Container as Apptainer\n\n    User-&gt;&gt;CLI: python main.py --recipe service.yaml\n    CLI-&gt;&gt;SSH: Connect to MeluXina\n    SSH-&gt;&gt;SLURM: sbatch job_script.sh\n    SLURM-&gt;&gt;Container: Launch on compute node\n    Container--&gt;&gt;SLURM: Service running\n    SLURM--&gt;&gt;SSH: Job ID + Node\n    SSH--&gt;&gt;CLI: Service info\n    CLI--&gt;&gt;User: Service started: abc123</code></pre>"},{"location":"getting-started/overview/#workflow","title":"Workflow","text":"<ol> <li>Configure - Set up <code>config.yaml</code> with HPC credentials</li> <li>Deploy - Start services using YAML recipes</li> <li>Benchmark - Run client workloads against services</li> <li>Monitor - View metrics in Grafana dashboards</li> <li>Analyze - Download and process results</li> </ol>"},{"location":"getting-started/overview/#next-steps","title":"Next Steps","text":"<ul> <li>Installation Guide - Detailed setup instructions</li> <li>Quick Start - Run your first benchmark in 5 minutes</li> <li>Architecture - Understand the system design</li> </ul> <p>Continue to Installation \u2192</p>"},{"location":"getting-started/quickstart/","title":"Quick Start","text":"<p>Run your first benchmark in 5 minutes! This guide walks through a complete end-to-end workflow.</p>"},{"location":"getting-started/quickstart/#prerequisites","title":"Prerequisites","text":"<ul> <li>Installation completed</li> <li>SSH connection working</li> <li>Container images available on MeluXina</li> </ul>"},{"location":"getting-started/quickstart/#option-1-automated-full-stack-recommended","title":"Option 1: Automated Full Stack (Recommended)","text":"<p>Use the automation scripts to start everything at once:</p> <pre><code># Start all services (Ollama, Redis, Chroma, MySQL, Prometheus, Grafana)\n./scripts/start_all_services.sh\n\n# Wait for services to be ready, then start benchmark clients\n./scripts/start_all_clients.sh\n</code></pre> <p>Then create SSH tunnels to access the dashboards:</p> <pre><code># Tunnel for Grafana (replace mel0XXX with actual node from script output)\nssh -i ~/.ssh/id_ed25519_mlux -L 3000:mel0XXX:3000 -N u103227@login.lxp.lu -p 8822\n\n# Tunnel for Prometheus\nssh -i ~/.ssh/id_ed25519_mlux -L 9090:mel0YYY:9090 -N u103227@login.lxp.lu -p 8822\n</code></pre> <p>Open http://localhost:3000 in your browser to see Grafana dashboards.</p>"},{"location":"getting-started/quickstart/#option-2-step-by-step-manual-workflow","title":"Option 2: Step-by-Step Manual Workflow","text":""},{"location":"getting-started/quickstart/#step-1-start-a-service","title":"Step 1: Start a Service","text":"<p>Start an Ollama LLM service:</p> <pre><code>python main.py --recipe recipes/services/ollama.yaml\n</code></pre> <p>Expected output:</p> <pre><code>Service started: ollama_a1b2c3d4\nMonitor the job status through SLURM or check logs.\n\n  To check status:\n    python main.py --status\n\n  To stop:\n    python main.py --stop-service ollama_a1b2c3d4\n</code></pre>"},{"location":"getting-started/quickstart/#step-2-check-status","title":"Step 2: Check Status","text":"<p>Wait for the service to start running:</p> <pre><code>python main.py --status\n</code></pre> <p>Expected output:</p> <pre><code>SLURM Job Status:\n  Total Jobs: 1\n  Services: 1\n  Clients: 0\n\nServices:\n  JOB_ID  | SERVICE_ID        | STATUS  | RUNTIME  | NODE\n  3656789 | ollama_a1b2c3d4   | RUNNING | 0:02:15  | mel2073\n</code></pre> <p>Note the Node</p> <p>Remember the node name (<code>mel2073</code> in this example) - you'll need it for client connections and SSH tunnels.</p>"},{"location":"getting-started/quickstart/#step-3-run-a-benchmark-client","title":"Step 3: Run a Benchmark Client","text":"<p>Run a benchmark against the service:</p> <pre><code># Using service ID (recommended)\npython main.py --recipe recipes/clients/ollama_benchmark.yaml --target-service ollama_a1b2c3d4\n\n# Or using direct endpoint\npython main.py --recipe recipes/clients/ollama_benchmark.yaml --target-endpoint http://mel2073:11434\n</code></pre> <p>Expected output:</p> <pre><code>Client started: ollama_bench_e5f6g7h8\nBenchmark running against: http://mel2073:11434\n</code></pre>"},{"location":"getting-started/quickstart/#step-4-monitor-progress","title":"Step 4: Monitor Progress","text":"<p>Check both service and client status:</p> <pre><code>python main.py --status\n</code></pre> <p>Expected output:</p> <pre><code>SLURM Job Status:\n  Total Jobs: 2\n  Services: 1\n  Clients: 1\n\nServices:\n  JOB_ID  | SERVICE_ID        | STATUS  | RUNTIME  | NODE\n  3656789 | ollama_a1b2c3d4   | RUNNING | 0:05:30  | mel2073\n\nClients:\n  JOB_ID  | CLIENT_ID              | STATUS  | RUNTIME  | NODE\n  3656790 | ollama_bench_e5f6g7h8  | RUNNING | 0:02:45  | mel2074\n</code></pre>"},{"location":"getting-started/quickstart/#step-5-view-results","title":"Step 5: View Results","text":""},{"location":"getting-started/quickstart/#option-a-download-results-locally","title":"Option A: Download Results Locally","text":"<pre><code># Download all results from cluster\npython main.py --download-results\n\n# View results\nls -la results/\ncat results/ollama_benchmark_*.json\n</code></pre>"},{"location":"getting-started/quickstart/#option-b-view-in-grafana","title":"Option B: View in Grafana","text":"<p>Create an SSH tunnel to access Grafana:</p> <pre><code># Start tunnel (replace mel0XXX with Grafana node)\nssh -i ~/.ssh/id_ed25519_mlux -L 3000:mel0XXX:3000 -N u103227@login.lxp.lu -p 8822\n</code></pre> <p>Open http://localhost:3000 in your browser.</p> <p>Default credentials: <code>admin</code> / <code>admin</code></p>"},{"location":"getting-started/quickstart/#step-6-clean-up","title":"Step 6: Clean Up","text":"<p>Stop services when done:</p> <pre><code># Stop a specific service\npython main.py --stop-service ollama_a1b2c3d4\n\n# Or stop all services\npython main.py --stop-all-services\n</code></pre> <p>Expected output:</p> <pre><code>\u2705 Stopped 2/2 services\n</code></pre>"},{"location":"getting-started/quickstart/#quick-command-reference","title":"Quick Command Reference","text":"Action Command Start service <code>python main.py --recipe recipes/services/ollama.yaml</code> Check status <code>python main.py --status</code> Run client <code>python main.py --recipe recipes/clients/ollama_benchmark.yaml --target-service &lt;ID&gt;</code> Stop service <code>python main.py --stop-service &lt;ID&gt;</code> Stop all <code>python main.py --stop-all-services</code> Download results <code>python main.py --download-results</code> List services <code>python main.py --list-services</code> List clients <code>python main.py --list-clients</code> Verbose mode <code>python main.py --verbose --status</code>"},{"location":"getting-started/quickstart/#example-redis-benchmark","title":"Example: Redis Benchmark","text":"<pre><code># 1. Start Redis service\npython main.py --recipe recipes/services/redis.yaml\n\n# 2. Check it's running\npython main.py --status\n\n# 3. Run benchmark\npython main.py --recipe recipes/clients/redis_benchmark.yaml --target-service redis_XXXX\n\n# 4. Download results\npython main.py --download-results\n</code></pre>"},{"location":"getting-started/quickstart/#example-full-monitoring-session","title":"Example: Full Monitoring Session","text":"<pre><code># Start service with cAdvisor monitoring\npython main.py --start-monitoring \\\n    recipes/services/ollama_with_cadvisor.yaml \\\n    recipes/services/prometheus_with_cadvisor.yaml\n\n# This starts:\n# - Ollama service with cAdvisor sidecar\n# - Prometheus scraping cAdvisor metrics\n# - Provides tunnel command for Prometheus UI\n</code></pre>"},{"location":"getting-started/quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>CLI Reference - All available commands</li> <li>Services Guide - Configure different services</li> <li>Monitoring Guide - Set up Grafana dashboards</li> <li>Recipes Guide - Write custom configurations</li> </ul>"},{"location":"monitoring/grafana/","title":"Grafana Dashboards","text":"<p>The orchestrator provides three pre-configured Grafana dashboards for monitoring services and benchmarks.</p>"},{"location":"monitoring/grafana/#accessing-grafana","title":"Accessing Grafana","text":""},{"location":"monitoring/grafana/#1-create-ssh-tunnel","title":"1. Create SSH Tunnel","text":"<pre><code># Find Grafana node from status or script output\npython main.py --status\n# Example: grafana_xxx | RUNNING | mel0164\n\n# Create tunnel\nssh -i ~/.ssh/id_ed25519_mlux -L 3000:mel0164:3000 -N u103227@login.lxp.lu -p 8822\n</code></pre>"},{"location":"monitoring/grafana/#2-open-browser","title":"2. Open Browser","text":"<p>Navigate to http://localhost:3000</p> <p>Default credentials: <code>admin</code> / <code>admin</code></p>"},{"location":"monitoring/grafana/#overview-dashboard","title":"Overview Dashboard","text":"<p>URL: <code>/d/overview/overview</code></p> <p>System-wide view of all monitored containers.</p>"},{"location":"monitoring/grafana/#panels","title":"Panels","text":"Panel Description Query Active Targets Count of UP scrape targets <code>count(up == 1)</code> Running Containers Number of containers <code>count(container_last_seen{name=~\".+\"})</code> Avg CPU % Average CPU usage <code>avg(rate(container_cpu_usage_seconds_total[1m])) * 100</code> Total Memory Sum of memory usage <code>sum(container_memory_usage_bytes)</code> Network RX Receive rate <code>sum(rate(container_network_receive_bytes_total[1m]))</code> Network TX Transmit rate <code>sum(rate(container_network_transmit_bytes_total[1m]))</code> CPU Timeline CPU usage over time <code>rate(container_cpu_usage_seconds_total{name=~\".+\"}[1m])</code> Memory Timeline Memory usage over time <code>container_memory_usage_bytes{name=~\".+\"}</code> Network Traffic Bidirectional traffic RX and TX combined Target Status Table of scrape targets <code>up</code>"},{"location":"monitoring/grafana/#service-monitoring-dashboard","title":"Service Monitoring Dashboard","text":"<p>URL: <code>/d/service-monitoring/service-monitoring</code></p> <p>Detailed metrics for selected containers.</p>"},{"location":"monitoring/grafana/#variables","title":"Variables","text":"Variable Description <code>$container</code> Multi-select container filter <code>$job</code> Multi-select job filter"},{"location":"monitoring/grafana/#panels_1","title":"Panels","text":"Panel Description CPU Bar Gauge Current CPU usage by container Memory Bar Gauge Current memory by container Network RX Bar Receive rate by container Network TX Bar Transmit rate by container CPU Timeline CPU over time with legend Memory Total Total memory timeline Memory Breakdown Working set + cache Network Throughput Bidirectional view Cumulative I/O Total bytes transferred Filesystem Usage Disk usage bars Memory Limit % Gauge showing % of limit"},{"location":"monitoring/grafana/#using-the-dashboard","title":"Using the Dashboard","text":"<ol> <li>Use the Container dropdown to filter by specific containers</li> <li>Use the Job dropdown to filter by Prometheus job</li> <li>Adjust time range in the top-right</li> <li>Click on legend items to show/hide series</li> </ol>"},{"location":"monitoring/grafana/#benchmark-dashboard","title":"Benchmark Dashboard","text":"<p>URL: <code>/d/benchmarks/benchmarks</code></p> <p>Performance-focused view during benchmark runs.</p>"},{"location":"monitoring/grafana/#panels_2","title":"Panels","text":"Panel Description Summary Stats Avg/Peak CPU and Memory Live CPU Timeline 30-second window for responsiveness Live Memory Timeline Current memory state Network RX Rate Receive throughput Network TX Rate Transmit throughput CPU Heatmap Visual CPU distribution Avg CPU Bar Average over benchmark period Avg Memory Bar Average over benchmark period Target Health Scrape target status"},{"location":"monitoring/grafana/#best-for","title":"Best for","text":"<ul> <li>Watching benchmark progress in real-time</li> <li>Comparing resource usage between containers</li> <li>Identifying performance bottlenecks</li> </ul>"},{"location":"monitoring/grafana/#customizing-dashboards","title":"Customizing Dashboards","text":""},{"location":"monitoring/grafana/#add-a-panel","title":"Add a Panel","text":"<ol> <li>Click Add panel button</li> <li>Choose visualization type</li> <li>Enter PromQL query</li> <li>Configure display options</li> <li>Save dashboard</li> </ol>"},{"location":"monitoring/grafana/#example-custom-panel","title":"Example Custom Panel","text":"<p>CPU usage gauge:</p> <pre><code>rate(container_cpu_usage_seconds_total{name=\"ollama\"}[1m]) * 100\n</code></pre> <p>Memory percentage:</p> <pre><code>container_memory_usage_bytes{name=\"ollama\"} / \ncontainer_spec_memory_limit_bytes{name=\"ollama\"} * 100\n</code></pre>"},{"location":"monitoring/grafana/#save-custom-dashboards","title":"Save Custom Dashboards","text":"<ol> <li>Make changes</li> <li>Click Save (disk icon)</li> <li>Optionally export as JSON</li> </ol>"},{"location":"monitoring/grafana/#useful-promql-queries","title":"Useful PromQL Queries","text":""},{"location":"monitoring/grafana/#per-container-cpu","title":"Per-Container CPU","text":"<pre><code>rate(container_cpu_usage_seconds_total{name=~\".+\"}[1m])\n</code></pre>"},{"location":"monitoring/grafana/#memory-working-set","title":"Memory Working Set","text":"<pre><code>container_memory_working_set_bytes{name=~\".+\"}\n</code></pre>"},{"location":"monitoring/grafana/#network-by-container","title":"Network by Container","text":"<pre><code># Receive rate\nrate(container_network_receive_bytes_total{name=~\".+\"}[1m])\n\n# Transmit rate (negative for bidirectional view)\n-rate(container_network_transmit_bytes_total{name=~\".+\"}[1m])\n</code></pre>"},{"location":"monitoring/grafana/#container-count","title":"Container Count","text":"<pre><code>count(container_last_seen{name=~\".+\"})\n</code></pre>"},{"location":"monitoring/grafana/#troubleshooting","title":"Troubleshooting","text":""},{"location":"monitoring/grafana/#no-data-displayed","title":"No Data Displayed","text":"<ol> <li>Check Prometheus is running and accessible</li> <li>Verify datasource configuration</li> <li>Ensure cAdvisor targets are being scraped</li> <li>Check time range includes recent data</li> </ol>"},{"location":"monitoring/grafana/#connection-refused","title":"Connection Refused","text":"<ol> <li>Verify SSH tunnel is active</li> <li>Check Grafana is running (<code>python main.py --status</code>)</li> <li>Confirm correct node in tunnel command</li> </ol>"},{"location":"monitoring/grafana/#datasource-error","title":"Datasource Error","text":"<ol> <li>Go to Configuration \u2192 Data Sources</li> <li>Click on Prometheus datasource</li> <li>Verify URL matches Prometheus node</li> <li>Click Test to verify connection</li> </ol> <p>See also: Monitoring Overview | Prometheus Metrics</p>"},{"location":"monitoring/overview/","title":"Monitoring Overview","text":"<p>The orchestrator provides comprehensive monitoring through Prometheus and Grafana, collecting real-time container metrics via cAdvisor.</p>"},{"location":"monitoring/overview/#architecture","title":"Architecture","text":"<pre><code>flowchart LR\n    subgraph Services[\"Deployed Services\"]\n        direction TB\n        Ollama[Ollama]\n        Redis[Redis]\n        Chroma[Chroma]\n        MySQL[MySQL]\n    end\n\n    subgraph Sidecars[\"Monitoring Sidecars\"]\n        direction TB\n        cA1[cAdvisor]\n        cA2[cAdvisor]\n        cA3[cAdvisor]\n        cA4[cAdvisor]\n    end\n\n    subgraph Stack[\"Monitoring Stack\"]\n        direction LR\n        Prometheus[Prometheus] --&gt; Grafana[Grafana]\n    end\n\n    Ollama --&gt; cA1\n    Redis --&gt; cA2\n    Chroma --&gt; cA3\n    MySQL --&gt; cA4\n\n    cA1 --&gt; Prometheus\n    cA2 --&gt; Prometheus\n    cA3 --&gt; Prometheus\n    cA4 --&gt; Prometheus\n\n    style Ollama fill:#0288D1,color:#fff\n    style Redis fill:#D32F2F,color:#fff\n    style Chroma fill:#689F38,color:#fff\n    style MySQL fill:#1565C0,color:#fff\n    style cA1 fill:#F57C00,color:#fff\n    style cA2 fill:#F57C00,color:#fff\n    style cA3 fill:#F57C00,color:#fff\n    style cA4 fill:#F57C00,color:#fff\n    style Prometheus fill:#E64A19,color:#fff\n    style Grafana fill:#F57C00,color:#fff</code></pre>"},{"location":"monitoring/overview/#components","title":"Components","text":""},{"location":"monitoring/overview/#cadvisor","title":"cAdvisor","text":"<p>Container Advisor collects resource usage metrics from containers:</p> <ul> <li>CPU usage and throttling</li> <li>Memory usage and limits</li> <li>Network I/O</li> <li>Filesystem usage</li> <li>Container lifecycle</li> </ul>"},{"location":"monitoring/overview/#prometheus","title":"Prometheus","text":"<p>Time-series database that:</p> <ul> <li>Scrapes cAdvisor endpoints every 15 seconds</li> <li>Stores metrics with configurable retention</li> <li>Provides PromQL query language</li> <li>Supports alerting (future)</li> </ul>"},{"location":"monitoring/overview/#grafana","title":"Grafana","text":"<p>Visualization platform with:</p> <ul> <li>Pre-built dashboards (Overview, Service, Benchmark)</li> <li>Real-time metric updates</li> <li>Customizable panels</li> <li>User-friendly interface</li> </ul>"},{"location":"monitoring/overview/#quick-start","title":"Quick Start","text":""},{"location":"monitoring/overview/#option-1-automated-script","title":"Option 1: Automated Script","text":"<pre><code># Start all services with monitoring\n./scripts/start_all_services.sh\n\n# Start benchmark clients\n./scripts/start_all_clients.sh\n\n# Create SSH tunnels (from script output)\nssh -L 3000:mel0164:3000 -N u103227@login.lxp.lu -p 8822  # Grafana\nssh -L 9090:mel0210:9090 -N u103227@login.lxp.lu -p 8822  # Prometheus\n\n# Open Grafana\nopen http://localhost:3000\n</code></pre>"},{"location":"monitoring/overview/#option-2-manual-setup","title":"Option 2: Manual Setup","text":"<pre><code># 1. Start services with cAdvisor\npython main.py --recipe recipes/services/ollama_with_cadvisor.yaml\npython main.py --recipe recipes/services/redis_with_cadvisor.yaml\n\n# 2. Start Prometheus (configure monitoring_targets)\npython main.py --recipe recipes/services/prometheus_with_cadvisor.yaml\n\n# 3. Start Grafana\npython main.py --recipe recipes/services/grafana.yaml\n\n# 4. Check status\npython main.py --status\n\n# 5. Create tunnels and access\n</code></pre>"},{"location":"monitoring/overview/#available-metrics","title":"Available Metrics","text":""},{"location":"monitoring/overview/#cpu-metrics","title":"CPU Metrics","text":"Metric Description <code>container_cpu_usage_seconds_total</code> Total CPU time consumed <code>container_cpu_system_seconds_total</code> System CPU time <code>container_cpu_user_seconds_total</code> User CPU time"},{"location":"monitoring/overview/#memory-metrics","title":"Memory Metrics","text":"Metric Description <code>container_memory_usage_bytes</code> Current memory usage <code>container_memory_working_set_bytes</code> Working set size <code>container_memory_cache</code> Page cache memory <code>container_spec_memory_limit_bytes</code> Memory limit"},{"location":"monitoring/overview/#network-metrics","title":"Network Metrics","text":"Metric Description <code>container_network_receive_bytes_total</code> Bytes received <code>container_network_transmit_bytes_total</code> Bytes transmitted <code>container_network_receive_packets_total</code> Packets received <code>container_network_transmit_packets_total</code> Packets transmitted"},{"location":"monitoring/overview/#filesystem-metrics","title":"Filesystem Metrics","text":"Metric Description <code>container_fs_usage_bytes</code> Filesystem bytes used <code>container_fs_limit_bytes</code> Filesystem size limit"},{"location":"monitoring/overview/#ssh-tunnels","title":"SSH Tunnels","text":"<p>Since HPC compute nodes aren't directly accessible, use SSH tunnels:</p> <pre><code># Grafana (port 3000)\nssh -i ~/.ssh/id_ed25519_mlux -L 3000:mel0164:3000 -N u103227@login.lxp.lu -p 8822\n\n# Prometheus (port 9090)\nssh -i ~/.ssh/id_ed25519_mlux -L 9090:mel0210:9090 -N u103227@login.lxp.lu -p 8822\n</code></pre> <p>Then access:</p> <ul> <li>Grafana: http://localhost:3000</li> <li>Prometheus: http://localhost:9090</li> </ul>"},{"location":"monitoring/overview/#querying-metrics","title":"Querying Metrics","text":""},{"location":"monitoring/overview/#via-cli","title":"Via CLI","text":"<pre><code>python main.py --query-metrics prometheus_xxx \"container_memory_usage_bytes\"\n</code></pre>"},{"location":"monitoring/overview/#via-prometheus-ui","title":"Via Prometheus UI","text":"<p>Navigate to http://localhost:9090 and enter PromQL queries.</p>"},{"location":"monitoring/overview/#via-grafana","title":"Via Grafana","text":"<p>Use the Explore feature or dashboard panels.</p> <p>Next: Grafana Dashboards | Prometheus Metrics</p>"},{"location":"monitoring/prometheus/","title":"Prometheus Metrics","text":"<p>Prometheus collects and stores metrics from cAdvisor sidecars running alongside services.</p>"},{"location":"monitoring/prometheus/#accessing-prometheus","title":"Accessing Prometheus","text":"<pre><code># Create SSH tunnel\nssh -i ~/.ssh/id_ed25519_mlux -L 9090:mel0210:9090 -N u103227@login.lxp.lu -p 8822\n\n# Open Prometheus UI\nopen http://localhost:9090\n</code></pre>"},{"location":"monitoring/prometheus/#configuration","title":"Configuration","text":"<p>Prometheus is automatically configured to scrape cAdvisor endpoints:</p> <pre><code># Auto-generated: $HOME/prometheus/config/prometheus.yml\nglobal:\n  scrape_interval: 15s\n  evaluation_interval: 15s\n\nscrape_configs:\n  - job_name: 'prometheus'\n    static_configs:\n      - targets: ['localhost:9090']\n\n  - job_name: 'ollama-cadvisor'\n    static_configs:\n      - targets: ['mel2073:8080']\n        labels:\n          service: 'ollama'\n          instance: 'mel2073'\n\n  - job_name: 'redis-cadvisor'\n    static_configs:\n      - targets: ['mel0182:8080']\n        labels:\n          service: 'redis'\n          instance: 'mel0182'\n</code></pre>"},{"location":"monitoring/prometheus/#promql-query-reference","title":"PromQL Query Reference","text":""},{"location":"monitoring/prometheus/#cpu-queries","title":"CPU Queries","text":"<pre><code># Instant CPU usage rate (per container)\nrate(container_cpu_usage_seconds_total{name=~\".+\"}[5m])\n\n# Average CPU across all containers\navg(rate(container_cpu_usage_seconds_total{name=~\".+\"}[5m]))\n\n# Max CPU usage\nmax(rate(container_cpu_usage_seconds_total{name=~\".+\"}[5m]))\n\n# CPU for specific container\nrate(container_cpu_usage_seconds_total{name=\"ollama\"}[5m])\n\n# CPU percentage (assuming 1 core = 100%)\nrate(container_cpu_usage_seconds_total{name=~\".+\"}[5m]) * 100\n</code></pre>"},{"location":"monitoring/prometheus/#memory-queries","title":"Memory Queries","text":"<pre><code># Current memory usage\ncontainer_memory_usage_bytes{name=~\".+\"}\n\n# Memory working set (more accurate for actual usage)\ncontainer_memory_working_set_bytes{name=~\".+\"}\n\n# Memory cache (page cache)\ncontainer_memory_cache{name=~\".+\"}\n\n# Memory as percentage of limit\ncontainer_memory_usage_bytes{name=~\".+\"} / \ncontainer_spec_memory_limit_bytes{name=~\".+\"} * 100\n\n# Total memory across all containers\nsum(container_memory_usage_bytes{name=~\".+\"})\n\n# Memory growth rate\nrate(container_memory_usage_bytes{name=~\".+\"}[5m])\n</code></pre>"},{"location":"monitoring/prometheus/#network-queries","title":"Network Queries","text":"<pre><code># Receive rate (bytes/sec)\nrate(container_network_receive_bytes_total{name=~\".+\"}[5m])\n\n# Transmit rate (bytes/sec)\nrate(container_network_transmit_bytes_total{name=~\".+\"}[5m])\n\n# Total bandwidth (RX + TX)\nrate(container_network_receive_bytes_total{name=~\".+\"}[5m]) +\nrate(container_network_transmit_bytes_total{name=~\".+\"}[5m])\n\n# Packet rate\nrate(container_network_receive_packets_total{name=~\".+\"}[5m])\n</code></pre>"},{"location":"monitoring/prometheus/#filesystem-queries","title":"Filesystem Queries","text":"<pre><code># Filesystem usage\ncontainer_fs_usage_bytes{name=~\".+\"}\n\n# Filesystem usage percentage\ncontainer_fs_usage_bytes{name=~\".+\"} / \ncontainer_fs_limit_bytes{name=~\".+\"} * 100\n</code></pre>"},{"location":"monitoring/prometheus/#target-status","title":"Target Status","text":"<pre><code># All targets up/down\nup\n\n# Only up targets\nup == 1\n\n# Count of up targets\ncount(up == 1)\n</code></pre>"},{"location":"monitoring/prometheus/#query-via-cli","title":"Query via CLI","text":"<pre><code># Simple query\npython main.py --query-metrics prometheus_xxx \"up\"\n\n# Container memory\npython main.py --query-metrics prometheus_xxx \"container_memory_usage_bytes\"\n\n# With label filter\npython main.py --query-metrics prometheus_xxx 'container_cpu_usage_seconds_total{name=\"ollama\"}'\n\n# Rate query\npython main.py --query-metrics prometheus_xxx 'rate(container_cpu_usage_seconds_total[5m])'\n</code></pre>"},{"location":"monitoring/prometheus/#metric-labels","title":"Metric Labels","text":""},{"location":"monitoring/prometheus/#common-labels","title":"Common Labels","text":"Label Description Example <code>name</code> Container name <code>ollama</code>, <code>redis</code> <code>instance</code> Node hostname <code>mel2073:8080</code> <code>job</code> Prometheus job name <code>ollama-cadvisor</code> <code>service</code> Service identifier <code>ollama</code>"},{"location":"monitoring/prometheus/#filtering-by-label","title":"Filtering by Label","text":"<pre><code># Specific container\ncontainer_memory_usage_bytes{name=\"ollama\"}\n\n# Multiple containers\ncontainer_memory_usage_bytes{name=~\"ollama|redis\"}\n\n# By job\ncontainer_cpu_usage_seconds_total{job=\"ollama-cadvisor\"}\n\n# Exclude pattern\ncontainer_memory_usage_bytes{name!~\"cadvisor\"}\n</code></pre>"},{"location":"monitoring/prometheus/#aggregation-functions","title":"Aggregation Functions","text":""},{"location":"monitoring/prometheus/#sum","title":"Sum","text":"<pre><code># Total memory\nsum(container_memory_usage_bytes{name=~\".+\"})\n\n# Sum by label\nsum by (name) (container_memory_usage_bytes{name=~\".+\"})\n</code></pre>"},{"location":"monitoring/prometheus/#average","title":"Average","text":"<pre><code># Average CPU\navg(rate(container_cpu_usage_seconds_total{name=~\".+\"}[5m]))\n</code></pre>"},{"location":"monitoring/prometheus/#maxmin","title":"Max/Min","text":"<pre><code># Peak memory\nmax(container_memory_usage_bytes{name=~\".+\"})\n\n# Min CPU\nmin(rate(container_cpu_usage_seconds_total{name=~\".+\"}[5m]))\n</code></pre>"},{"location":"monitoring/prometheus/#count","title":"Count","text":"<pre><code># Number of containers\ncount(container_last_seen{name=~\".+\"})\n</code></pre>"},{"location":"monitoring/prometheus/#time-functions","title":"Time Functions","text":""},{"location":"monitoring/prometheus/#rate","title":"Rate","text":"<pre><code># Per-second rate over 5 minutes\nrate(container_cpu_usage_seconds_total[5m])\n\n# Use $__rate_interval in Grafana for auto-adjustment\nrate(container_cpu_usage_seconds_total[$__rate_interval])\n</code></pre>"},{"location":"monitoring/prometheus/#increase","title":"Increase","text":"<pre><code># Total increase over 1 hour\nincrease(container_network_receive_bytes_total[1h])\n</code></pre>"},{"location":"monitoring/prometheus/#delta","title":"Delta","text":"<pre><code># Change over 5 minutes\ndelta(container_memory_usage_bytes[5m])\n</code></pre>"},{"location":"monitoring/prometheus/#data-retention","title":"Data Retention","text":"<p>Configure retention in Prometheus recipe:</p> <pre><code>environment:\n  PROMETHEUS_RETENTION_TIME: \"15d\"  # Keep 15 days\n  PROMETHEUS_STORAGE_PATH: \"/prometheus\"\n</code></pre> <p>See also: Monitoring Overview | Grafana Dashboards</p>"},{"location":"recipes/clients/","title":"Client Recipes","text":"<p>Client recipes define benchmark workloads that run against deployed services.</p>"},{"location":"recipes/clients/#available-client-recipes","title":"Available Client Recipes","text":"Recipe Target Service Type <code>ollama_benchmark.yaml</code> Ollama Single run <code>ollama_parametric.yaml</code> Ollama Parametric sweep <code>redis_benchmark.yaml</code> Redis Single run <code>redis_parametric.yaml</code> Redis Parametric sweep <code>chroma_benchmark.yaml</code> Chroma Single run <code>chroma_parametric.yaml</code> Chroma Parametric sweep <code>mysql_benchmark.yaml</code> MySQL Single run"},{"location":"recipes/clients/#recipe-fields-reference","title":"Recipe Fields Reference","text":""},{"location":"recipes/clients/#required-fields","title":"Required Fields","text":"Field Type Description <code>name</code> string Client identifier <code>type</code> string Benchmark type (e.g., <code>ollama_benchmark</code>)"},{"location":"recipes/clients/#parameter-fields","title":"Parameter Fields","text":"Field Type Description <code>parameters.*</code> various Benchmark-specific parameters <code>parameters.output_file</code> string Results output path"},{"location":"recipes/clients/#resource-fields","title":"Resource Fields","text":"<p>Same as service recipes, but typically lighter:</p> Field Default Description <code>cpus_per_task</code> 2 CPUs for benchmark <code>mem</code> \"4G\" Memory allocation <code>time</code> \"00:30:00\" Time limit <code>partition</code> \"cpu\" Usually CPU partition"},{"location":"recipes/clients/#example-ollama-benchmark","title":"Example: Ollama Benchmark","text":"<pre><code># recipes/clients/ollama_benchmark.yaml\nclient:\n  name: ollama_benchmark\n  type: ollama_benchmark\n  description: \"LLM inference benchmark\"\n\n  parameters:\n    model: \"llama2\"\n    num_requests: 50\n    concurrent_requests: 5\n    max_tokens: 100\n    temperature: 0.7\n    output_file: \"$HOME/results/ollama_benchmark.json\"\n\n  resources:\n    cpus_per_task: 2\n    mem: \"4G\"\n    time: \"00:30:00\"\n    partition: cpu\n</code></pre>"},{"location":"recipes/clients/#example-redis-benchmark","title":"Example: Redis Benchmark","text":"<pre><code># recipes/clients/redis_benchmark.yaml\nclient:\n  name: redis_benchmark\n  type: redis_benchmark\n  description: \"Redis performance benchmark\"\n\n  parameters:\n    clients: 50\n    requests: 100000\n    data_size: 256\n    pipeline: 1\n    tests: \"SET,GET,LPUSH,LPOP,SADD,SPOP\"\n    output_file: \"$HOME/results/redis_benchmark.json\"\n\n  resources:\n    cpus_per_task: 4\n    mem: \"4G\"\n    time: \"00:30:00\"\n    partition: cpu\n</code></pre>"},{"location":"recipes/clients/#parametric-benchmarks","title":"Parametric Benchmarks","text":"<p>Parametric recipes test across multiple configurations:</p>"},{"location":"recipes/clients/#redis-parametric","title":"Redis Parametric","text":"<pre><code># recipes/clients/redis_parametric.yaml\nclient:\n  name: redis_parametric\n  type: redis_parametric_benchmark\n  description: \"Comprehensive Redis performance sweep\"\n\n  parameters:\n    # Parameter ranges to sweep\n    client_counts: [1, 10, 50, 100, 200, 500]\n    data_sizes: [64, 256, 1024, 4096, 16384, 65536]\n    pipeline_depths: [1, 10, 50, 100, 256]\n    requests_per_config: 100000\n    tests: \"SET,GET\"\n    output_dir: \"$HOME/results/redis_parametric/\"\n\n  resources:\n    cpus_per_task: 4\n    mem: \"8G\"\n    time: \"02:00:00\"\n    partition: cpu\n</code></pre>"},{"location":"recipes/clients/#ollama-parametric","title":"Ollama Parametric","text":"<pre><code># recipes/clients/ollama_parametric.yaml\nclient:\n  name: ollama_parametric\n  type: ollama_parametric_benchmark\n\n  parameters:\n    models: [\"llama2\", \"mistral\", \"codellama\"]\n    concurrent_requests: [1, 2, 4, 8]\n    max_tokens: [50, 100, 200]\n    num_requests_per_config: 20\n    output_dir: \"$HOME/results/ollama_parametric/\"\n</code></pre>"},{"location":"recipes/clients/#using-client-recipes","title":"Using Client Recipes","text":""},{"location":"recipes/clients/#with-target-service-id","title":"With Target Service ID","text":"<pre><code># Start service first\npython main.py --recipe recipes/services/ollama.yaml\n# Output: Service started: ollama_abc123\n\n# Run client with service ID\npython main.py --recipe recipes/clients/ollama_benchmark.yaml --target-service ollama_abc123\n</code></pre>"},{"location":"recipes/clients/#with-direct-endpoint","title":"With Direct Endpoint","text":"<pre><code># If you know the node\npython main.py --recipe recipes/clients/ollama_benchmark.yaml --target-endpoint http://mel2073:11434\n</code></pre>"},{"location":"recipes/clients/#running-parametric-sweeps","title":"Running Parametric Sweeps","text":"<p>Use the automation scripts:</p> <pre><code># Redis parametric (tests all combinations)\n./scripts/run_redis_parametric.sh\n\n# Ollama parametric\n./scripts/run_ollama_parametric.sh\n\n# Chroma parametric\n./scripts/run_chroma_parametric.sh\n</code></pre>"},{"location":"recipes/clients/#benchmark-output-format","title":"Benchmark Output Format","text":"<p>All benchmarks produce JSON output:</p> <pre><code>{\n  \"benchmark\": \"redis_benchmark\",\n  \"timestamp\": \"2026-01-15T14:30:22Z\",\n  \"target\": \"mel0182:6379\",\n  \"config\": {\n    \"clients\": 50,\n    \"requests\": 100000,\n    \"data_size\": 256\n  },\n  \"results\": {\n    \"SET\": {\n      \"ops_per_second\": 125000,\n      \"latency_avg_ms\": 0.4,\n      \"latency_p50_ms\": 0.35,\n      \"latency_p95_ms\": 0.8,\n      \"latency_p99_ms\": 1.2\n    },\n    \"GET\": {\n      \"ops_per_second\": 145000,\n      \"latency_avg_ms\": 0.35,\n      \"latency_p50_ms\": 0.3,\n      \"latency_p95_ms\": 0.7,\n      \"latency_p99_ms\": 1.0\n    }\n  },\n  \"summary\": {\n    \"total_operations\": 200000,\n    \"total_duration_seconds\": 1.5,\n    \"success_rate\": 100.0\n  }\n}\n</code></pre>"},{"location":"recipes/clients/#results-analysis","title":"Results Analysis","text":"<p>After running benchmarks:</p> <pre><code># Download results\npython main.py --download-results\n\n# Analyze with provided scripts\npython analysis/plot_redis_results.py\npython analysis/plot_ollama_results.py\n</code></pre> <p>Next: Writing Custom Recipes</p>"},{"location":"recipes/custom/","title":"Writing Custom Recipes","text":"<p>Create custom recipes for new services or specialized benchmark configurations.</p>"},{"location":"recipes/custom/#creating-a-service-recipe","title":"Creating a Service Recipe","text":""},{"location":"recipes/custom/#step-1-basic-structure","title":"Step 1: Basic Structure","text":"<pre><code># recipes/services/my_service.yaml\nservice:\n  name: my_service\n  description: \"Description of my service\"\n\n  container:\n    docker_source: docker://myimage:tag\n    image_path: $HOME/containers/my_service.sif\n\n  resources:\n    cpus_per_task: 4\n    mem: \"8G\"\n    time: \"02:00:00\"\n    partition: cpu\n\n  environment:\n    MY_VAR: \"value\"\n\n  ports:\n    - 8080\n</code></pre>"},{"location":"recipes/custom/#step-2-test-the-recipe","title":"Step 2: Test the Recipe","text":"<pre><code># Validate and run\npython main.py --verbose --recipe recipes/services/my_service.yaml\n\n# Check it started\npython main.py --status\n</code></pre>"},{"location":"recipes/custom/#step-3-add-monitoring-optional","title":"Step 3: Add Monitoring (Optional)","text":"<pre><code>service:\n  name: my_service\n  # ... other fields ...\n\n  enable_cadvisor: true\n  cadvisor_port: 8080\n</code></pre>"},{"location":"recipes/custom/#creating-a-client-recipe","title":"Creating a Client Recipe","text":""},{"location":"recipes/custom/#step-1-choose-client-type","title":"Step 1: Choose Client Type","text":"<p>Use an existing client type or create a new one:</p> Type For Service <code>ollama_benchmark</code> Ollama LLM <code>redis_benchmark</code> Redis <code>chroma_benchmark</code> Chroma <code>mysql_benchmark</code> MySQL"},{"location":"recipes/custom/#step-2-define-parameters","title":"Step 2: Define Parameters","text":"<pre><code># recipes/clients/my_benchmark.yaml\nclient:\n  name: my_benchmark\n  type: redis_benchmark  # Use existing type\n\n  parameters:\n    clients: 100\n    requests: 50000\n    data_size: 512\n    tests: \"SET,GET,HSET,HGET\"\n    output_file: \"$HOME/results/my_benchmark.json\"\n\n  resources:\n    cpus_per_task: 2\n    mem: \"4G\"\n    time: \"00:30:00\"\n</code></pre>"},{"location":"recipes/custom/#step-3-run-the-benchmark","title":"Step 3: Run the Benchmark","text":"<pre><code>python main.py --recipe recipes/clients/my_benchmark.yaml --target-service redis_xxx\n</code></pre>"},{"location":"recipes/custom/#advanced-custom-service-class","title":"Advanced: Custom Service Class","text":"<p>For completely new services, create a Python class:</p>"},{"location":"recipes/custom/#step-1-create-service-class","title":"Step 1: Create Service Class","text":"<pre><code># src/services/my_service.py\nfrom .base import Service\nfrom ..base import JobFactory\n\nclass MyService(Service):\n    \"\"\"My custom service implementation\"\"\"\n\n    def __init__(self, config: dict):\n        super().__init__(config)\n        self.my_option = config.get('my_option', 'default')\n\n    def get_service_setup_commands(self):\n        \"\"\"Custom setup commands\"\"\"\n        commands = super().get_service_setup_commands()\n        commands.extend([\n            f\"export MY_OPTION={self.my_option}\",\n            \"mkdir -p $HOME/my_service/data\",\n        ])\n        return commands\n\n    def get_container_command(self):\n        \"\"\"Custom container execution\"\"\"\n        return f\"\"\"\n        apptainer exec \\\\\n            --bind $HOME/my_service/data:/data \\\\\n            {self._resolve_container_path()} \\\\\n            my_service_command --port 8080 &amp;\n        \"\"\"\n\n# Register with factory\nJobFactory.register_service('my_service', MyService)\n</code></pre>"},{"location":"recipes/custom/#step-2-register-in-__init__py","title":"Step 2: Register in <code>__init__.py</code>","text":"<pre><code># src/services/__init__.py\nfrom .my_service import MyService\n# Add to imports and registration\n</code></pre>"},{"location":"recipes/custom/#step-3-create-recipe","title":"Step 3: Create Recipe","text":"<pre><code># recipes/services/my_service.yaml\nservice:\n  name: my_service\n  my_option: \"custom_value\"\n\n  container:\n    docker_source: docker://myimage:tag\n    image_path: $HOME/containers/my_service.sif\n\n  resources:\n    cpus_per_task: 4\n    mem: \"8G\"\n</code></pre>"},{"location":"recipes/custom/#recipe-validation","title":"Recipe Validation","text":"<p>Recipes are validated for:</p> <ol> <li>Required fields: <code>name</code>, <code>container.docker_source</code></li> <li>Valid types: Resource values, partition names</li> <li>Path resolution: <code>$HOME</code>, <code>$SCRATCH</code> variables</li> <li>Port conflicts: Unique port assignments</li> </ol>"},{"location":"recipes/custom/#tips-for-custom-recipes","title":"Tips for Custom Recipes","text":"<p>Start Simple</p> <p>Begin with minimal configuration and add options as needed.</p> <p>Test Locally First</p> <p>If possible, test container commands locally before deploying.</p> <p>Use Existing Templates</p> <p>Copy and modify existing recipes rather than starting from scratch.</p> <p>Resource Limits</p> <p>Be mindful of HPC allocation limits when setting resources.</p>"},{"location":"recipes/custom/#debugging-recipes","title":"Debugging Recipes","text":"<pre><code># Verbose mode shows generated script\npython main.py --verbose --recipe recipes/services/my_service.yaml\n\n# View generated script\ncat scripts/service_my_service_*.sh\n\n# Check SLURM logs\nssh meluxina \"cat slurm-*.out\"\n</code></pre> <p>See also: Services Overview | Development Guide</p>"},{"location":"recipes/overview/","title":"Recipes Overview","text":"<p>Recipes are YAML configuration files that define how services and clients are deployed on the HPC cluster.</p>"},{"location":"recipes/overview/#what-are-recipes","title":"What Are Recipes?","text":"<p>Recipes provide a declarative way to specify:</p> <ul> <li>Container configuration: Image source and path</li> <li>Resource allocation: CPU, memory, GPU, time limits</li> <li>Environment variables: Service-specific settings</li> <li>Network ports: Exposed endpoints</li> <li>Monitoring: cAdvisor integration</li> </ul>"},{"location":"recipes/overview/#recipe-types","title":"Recipe Types","text":"Type Location Purpose Service <code>recipes/services/</code> Deploy persistent services (Ollama, Redis, etc.) Client <code>recipes/clients/</code> Run benchmark workloads"},{"location":"recipes/overview/#directory-structure","title":"Directory Structure","text":"<pre><code>recipes/\n\u251c\u2500\u2500 services/\n\u2502   \u251c\u2500\u2500 ollama.yaml\n\u2502   \u251c\u2500\u2500 ollama_with_cadvisor.yaml\n\u2502   \u251c\u2500\u2500 redis.yaml\n\u2502   \u251c\u2500\u2500 redis_with_cadvisor.yaml\n\u2502   \u251c\u2500\u2500 chroma.yaml\n\u2502   \u251c\u2500\u2500 chroma_with_cadvisor.yaml\n\u2502   \u251c\u2500\u2500 mysql.yaml\n\u2502   \u251c\u2500\u2500 mysql_with_cadvisor.yaml\n\u2502   \u251c\u2500\u2500 prometheus_with_cadvisor.yaml\n\u2502   \u2514\u2500\u2500 grafana.yaml\n\u2514\u2500\u2500 clients/\n    \u251c\u2500\u2500 ollama_benchmark.yaml\n    \u251c\u2500\u2500 ollama_parametric.yaml\n    \u251c\u2500\u2500 redis_benchmark.yaml\n    \u251c\u2500\u2500 redis_parametric.yaml\n    \u251c\u2500\u2500 chroma_benchmark.yaml\n    \u251c\u2500\u2500 chroma_parametric.yaml\n    \u2514\u2500\u2500 mysql_benchmark.yaml\n</code></pre>"},{"location":"recipes/overview/#basic-recipe-structure","title":"Basic Recipe Structure","text":""},{"location":"recipes/overview/#service-recipe","title":"Service Recipe","text":"<pre><code>service:\n  name: service_name\n  description: \"Human-readable description\"\n\n  # Container image\n  container:\n    docker_source: docker://image:tag\n    image_path: $HOME/containers/image.sif\n\n  # SLURM resources\n  resources:\n    nodes: 1\n    ntasks: 1\n    cpus_per_task: 4\n    mem: \"16G\"\n    time: \"02:00:00\"\n    partition: gpu\n    qos: default\n    gres: \"gpu:1\"\n\n  # Environment variables\n  environment:\n    VAR_NAME: \"value\"\n\n  # Network\n  ports:\n    - 8080\n\n  # Monitoring (optional)\n  enable_cadvisor: true\n  cadvisor_port: 8080\n</code></pre>"},{"location":"recipes/overview/#client-recipe","title":"Client Recipe","text":"<pre><code>client:\n  name: client_name\n  type: benchmark_type\n  description: \"Benchmark description\"\n\n  # Benchmark parameters\n  parameters:\n    param1: value1\n    param2: value2\n    output_file: \"$HOME/results/output.json\"\n\n  # SLURM resources\n  resources:\n    cpus_per_task: 2\n    mem: \"4G\"\n    time: \"00:30:00\"\n    partition: cpu\n</code></pre>"},{"location":"recipes/overview/#using-recipes","title":"Using Recipes","text":""},{"location":"recipes/overview/#start-a-service","title":"Start a Service","text":"<pre><code>python main.py --recipe recipes/services/ollama.yaml\n</code></pre>"},{"location":"recipes/overview/#run-a-client","title":"Run a Client","text":"<pre><code>python main.py --recipe recipes/clients/ollama_benchmark.yaml --target-service ollama_xxx\n</code></pre>"},{"location":"recipes/overview/#with-verbose-output","title":"With Verbose Output","text":"<pre><code>python main.py --verbose --recipe recipes/services/redis.yaml\n</code></pre>"},{"location":"recipes/overview/#recipe-processing-flow","title":"Recipe Processing Flow","text":"<pre><code>sequenceDiagram\n    participant User\n    participant CLI as main.py\n    participant Loader as RecipeLoader\n    participant Factory as JobFactory\n    participant Service as ServiceClass\n    participant Generator as ScriptGenerator\n    participant SSH\n\n    User-&gt;&gt;CLI: --recipe service.yaml\n    CLI-&gt;&gt;Loader: load(path)\n    Loader--&gt;&gt;CLI: recipe dict\n    CLI-&gt;&gt;Factory: create_service(recipe)\n    Factory-&gt;&gt;Service: __init__(recipe)\n    Service--&gt;&gt;Factory: instance\n    Factory--&gt;&gt;CLI: service\n    CLI-&gt;&gt;Generator: generate_script(service)\n    Generator--&gt;&gt;CLI: slurm_script\n    CLI-&gt;&gt;SSH: submit_job(script)\n    SSH--&gt;&gt;CLI: job_id\n    CLI--&gt;&gt;User: Service started</code></pre>"},{"location":"recipes/overview/#validation","title":"Validation","text":"<p>Recipes are validated for:</p> <ul> <li>Required fields (name, container)</li> <li>Valid resource values</li> <li>Correct YAML syntax</li> <li>Environment variable format</li> </ul> <p>Next: Service Recipes | Client Recipes</p>"},{"location":"recipes/services/","title":"Service Recipes","text":"<p>Service recipes define how AI and database services are deployed on the HPC cluster.</p>"},{"location":"recipes/services/#available-service-recipes","title":"Available Service Recipes","text":"Recipe Service GPU Monitoring <code>ollama.yaml</code> Ollama LLM Yes No <code>ollama_with_cadvisor.yaml</code> Ollama LLM Yes Yes <code>redis.yaml</code> Redis No No <code>redis_with_cadvisor.yaml</code> Redis No Yes <code>chroma.yaml</code> Chroma No No <code>chroma_with_cadvisor.yaml</code> Chroma No Yes <code>mysql.yaml</code> MySQL No No <code>mysql_with_cadvisor.yaml</code> MySQL No Yes <code>prometheus_with_cadvisor.yaml</code> Prometheus No Yes <code>grafana.yaml</code> Grafana No No"},{"location":"recipes/services/#recipe-fields-reference","title":"Recipe Fields Reference","text":""},{"location":"recipes/services/#required-fields","title":"Required Fields","text":"Field Type Description <code>name</code> string Unique service identifier <code>container.docker_source</code> string Docker image source <code>container.image_path</code> string Local SIF path"},{"location":"recipes/services/#resource-fields","title":"Resource Fields","text":"Field Type Default Description <code>resources.nodes</code> int 1 Number of nodes <code>resources.ntasks</code> int 1 Number of tasks <code>resources.cpus_per_task</code> int 1 CPUs per task <code>resources.mem</code> string \"4G\" Memory allocation <code>resources.time</code> string \"01:00:00\" Time limit <code>resources.partition</code> string \"cpu\" SLURM partition <code>resources.qos</code> string \"default\" Quality of service <code>resources.gres</code> string - GPU resources"},{"location":"recipes/services/#optional-fields","title":"Optional Fields","text":"Field Type Description <code>description</code> string Human-readable description <code>environment</code> dict Environment variables <code>ports</code> list Exposed ports <code>enable_cadvisor</code> bool Enable monitoring <code>cadvisor_port</code> int cAdvisor port (default: 8080) <code>command</code> string Override container command <code>args</code> list Command arguments"},{"location":"recipes/services/#example-ollama-service","title":"Example: Ollama Service","text":"<pre><code># recipes/services/ollama.yaml\nservice:\n  name: ollama\n  description: \"Ollama LLM inference server with GPU acceleration\"\n\n  container:\n    docker_source: docker://ollama/ollama:latest\n    image_path: $HOME/containers/ollama_latest.sif\n\n  resources:\n    nodes: 1\n    ntasks: 1\n    cpus_per_task: 4\n    mem: \"32G\"\n    time: \"04:00:00\"\n    partition: gpu\n    qos: default\n    gres: \"gpu:1\"\n\n  environment:\n    OLLAMA_HOST: \"0.0.0.0:11434\"\n    OLLAMA_MODELS: \"$HOME/.ollama/models\"\n    OLLAMA_NUM_PARALLEL: \"4\"\n    OLLAMA_KEEP_ALIVE: \"5m\"\n\n  ports:\n    - 11434\n</code></pre>"},{"location":"recipes/services/#example-redis-with-monitoring","title":"Example: Redis with Monitoring","text":"<pre><code># recipes/services/redis_with_cadvisor.yaml\nservice:\n  name: redis\n  description: \"Redis in-memory database with cAdvisor monitoring\"\n\n  container:\n    docker_source: docker://redis:latest\n    image_path: $HOME/containers/redis_latest.sif\n\n  resources:\n    nodes: 1\n    cpus_per_task: 4\n    mem: \"8G\"\n    time: \"02:00:00\"\n    partition: cpu\n\n  environment:\n    REDIS_PORT: \"6379\"\n    REDIS_BIND: \"0.0.0.0\"\n\n  ports:\n    - 6379\n\n  # Enable cAdvisor sidecar\n  enable_cadvisor: true\n  cadvisor_port: 8080\n</code></pre>"},{"location":"recipes/services/#example-prometheus","title":"Example: Prometheus","text":"<pre><code># recipes/services/prometheus_with_cadvisor.yaml\nservice:\n  name: prometheus\n  description: \"Prometheus metrics collection\"\n\n  container:\n    docker_source: docker://prom/prometheus:latest\n    image_path: $HOME/containers/prometheus.sif\n\n  # Services to monitor (resolved at runtime)\n  monitoring_targets:\n    - service_id: \"ollama_abc123\"\n      job_name: \"ollama-cadvisor\"\n      port: 8080\n    - service_id: \"redis_xyz789\"\n      job_name: \"redis-cadvisor\"\n      port: 8080\n\n  resources:\n    cpus_per_task: 2\n    mem: \"4G\"\n    time: \"02:00:00\"\n    partition: cpu\n\n  environment:\n    PROMETHEUS_RETENTION_TIME: \"15d\"\n\n  ports:\n    - 9090\n</code></pre>"},{"location":"recipes/services/#using-service-recipes","title":"Using Service Recipes","text":"<pre><code># Basic usage\npython main.py --recipe recipes/services/ollama.yaml\n\n# With verbose output\npython main.py --verbose --recipe recipes/services/redis.yaml\n\n# Check what's running\npython main.py --status\n</code></pre>"},{"location":"recipes/services/#generated-slurm-script","title":"Generated SLURM Script","text":"<p>A service recipe generates a SLURM script like:</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=ollama_a1b2c3d4\n#SBATCH --account=p200981\n#SBATCH --partition=gpu\n#SBATCH --qos=default\n#SBATCH --nodes=1\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=4\n#SBATCH --mem=32G\n#SBATCH --time=04:00:00\n#SBATCH --gres=gpu:1\n#SBATCH --output=slurm-%j.out\n#SBATCH --error=slurm-%j.err\n\nmodule purge\nmodule load Apptainer/1.2.4-GCCcore-12.3.0\n\n# Service setup\nexport OLLAMA_HOST=0.0.0.0:11434\nexport OLLAMA_MODELS=$HOME/.ollama/models\nmkdir -p $HOME/.ollama/models\n\n# Container execution\napptainer exec --nv \\\n    --bind $HOME/.ollama:/root/.ollama \\\n    $HOME/containers/ollama_latest.sif \\\n    ollama serve &amp;\n\n# Health check\nsleep 10\ncurl -s http://localhost:11434/api/tags\n\nwait\n</code></pre> <p>Next: Client Recipes | Writing Custom Recipes</p>"},{"location":"services/chroma/","title":"Chroma Service","text":"<p>Chroma is an open-source vector database for AI applications.</p>"},{"location":"services/chroma/#overview","title":"Overview","text":"Property Value Type Vector Database Default Port 8000 GPU Required No Container <code>docker://chromadb/chroma:latest</code>"},{"location":"services/chroma/#quick-start","title":"Quick Start","text":"<pre><code># Start Chroma service\npython main.py --recipe recipes/services/chroma.yaml\n\n# Run benchmark\npython main.py --recipe recipes/clients/chroma_benchmark.yaml --target-service chroma_xxx\n</code></pre>"},{"location":"services/chroma/#recipe-configuration","title":"Recipe Configuration","text":"<pre><code># recipes/services/chroma.yaml\nservice:\n  name: chroma\n  description: \"Chroma vector database\"\n\n  container:\n    docker_source: docker://chromadb/chroma:latest\n    image_path: $HOME/containers/chroma_latest.sif\n\n  resources:\n    nodes: 1\n    ntasks: 1\n    cpus_per_task: 4\n    mem: \"16G\"\n    time: \"02:00:00\"\n    partition: cpu\n    qos: default\n\n  environment:\n    CHROMA_SERVER_HOST: \"0.0.0.0\"\n    CHROMA_SERVER_PORT: \"8000\"\n    PERSIST_DIRECTORY: \"/data\"\n\n  ports:\n    - 8000\n</code></pre>"},{"location":"services/chroma/#benchmark-client","title":"Benchmark Client","text":"<pre><code># recipes/clients/chroma_benchmark.yaml\nclient:\n  name: chroma_benchmark\n  type: chroma_benchmark\n\n  parameters:\n    collection_name: \"benchmark_collection\"\n    num_documents: 10000\n    embedding_dimension: 384\n    num_queries: 1000\n    top_k: 10\n    output_file: \"$HOME/results/chroma_benchmark.json\"\n</code></pre>"},{"location":"services/chroma/#benchmark-operations","title":"Benchmark Operations","text":"Operation Description <code>insert</code> Add documents with embeddings <code>query</code> Similarity search <code>update</code> Modify existing documents <code>delete</code> Remove documents"},{"location":"services/chroma/#metrics","title":"Metrics","text":"Metric Description <code>insert_throughput</code> Documents inserted per second <code>query_latency_avg</code> Average query time <code>query_latency_p99</code> 99th percentile query time <code>recall@k</code> Search accuracy"},{"location":"services/chroma/#api-examples","title":"API Examples","text":"<pre><code>import chromadb\n\n# Connect\nclient = chromadb.HttpClient(host=\"mel0058\", port=8000)\n\n# Create collection\ncollection = client.create_collection(\"my_collection\")\n\n# Add documents\ncollection.add(\n    documents=[\"doc1\", \"doc2\"],\n    embeddings=[[0.1, 0.2, ...], [0.3, 0.4, ...]],\n    ids=[\"id1\", \"id2\"]\n)\n\n# Query\nresults = collection.query(\n    query_embeddings=[[0.1, 0.2, ...]],\n    n_results=5\n)\n</code></pre> <p>See also: Services Overview</p>"},{"location":"services/grafana/","title":"Grafana Service","text":"<p>Grafana provides visualization dashboards for monitoring metrics.</p>"},{"location":"services/grafana/#overview","title":"Overview","text":"Property Value Type Visualization Default Port 3000 GPU Required No Container <code>docker://grafana/grafana:latest</code>"},{"location":"services/grafana/#quick-start","title":"Quick Start","text":"<pre><code># Start full monitoring stack\n./scripts/start_all_services.sh\n\n# Create tunnel\nssh -L 3000:mel0164:3000 -N u103227@login.lxp.lu -p 8822\n\n# Access Grafana\nopen http://localhost:3000\n# Default: admin / admin\n</code></pre>"},{"location":"services/grafana/#recipe-configuration","title":"Recipe Configuration","text":"<pre><code># recipes/services/grafana.yaml\nservice:\n  name: grafana\n  description: \"Grafana visualization\"\n\n  container:\n    docker_source: docker://grafana/grafana:latest\n    image_path: $HOME/containers/grafana.sif\n\n  resources:\n    nodes: 1\n    cpus_per_task: 1\n    mem: \"1G\"\n    time: \"02:00:00\"\n    partition: cpu\n\n  environment:\n    GF_SECURITY_ADMIN_USER: \"admin\"\n    GF_SECURITY_ADMIN_PASSWORD: \"admin\"\n    GF_AUTH_ANONYMOUS_ENABLED: \"true\"\n    GF_AUTH_ANONYMOUS_ORG_ROLE: \"Viewer\"\n\n  ports:\n    - 3000\n</code></pre>"},{"location":"services/grafana/#pre-built-dashboards","title":"Pre-Built Dashboards","text":"<p>The orchestrator provides three pre-configured dashboards:</p>"},{"location":"services/grafana/#1-overview-dashboard","title":"1. Overview Dashboard","text":"<p>Path: <code>/d/overview/overview</code></p> <p>Displays system-wide metrics:</p> <ul> <li>Active targets count</li> <li>Running containers</li> <li>Total CPU/Memory usage</li> <li>CPU usage by container (time series)</li> <li>Memory usage by container (time series)</li> <li>Network traffic (RX/TX)</li> <li>Scrape target status table</li> </ul>"},{"location":"services/grafana/#2-service-monitoring-dashboard","title":"2. Service Monitoring Dashboard","text":"<p>Path: <code>/d/service-monitoring/service-monitoring</code></p> <p>Detailed per-service metrics with container selector:</p> <ul> <li>Resource overview (CPU, Memory, Network bars)</li> <li>CPU usage timeline</li> <li>Memory breakdown (total, working set, cache)</li> <li>Network throughput</li> <li>Filesystem usage</li> <li>Memory limit gauge</li> </ul>"},{"location":"services/grafana/#3-benchmark-dashboard","title":"3. Benchmark Dashboard","text":"<p>Path: <code>/d/benchmarks/benchmarks</code></p> <p>Performance-focused view during benchmark runs:</p> <ul> <li>Summary statistics (Avg/Peak CPU, Memory)</li> <li>Live CPU timeline (30s window)</li> <li>Live memory timeline</li> <li>Network performance</li> <li>CPU heatmap</li> <li>Resource comparison bars</li> </ul>"},{"location":"services/grafana/#accessing-dashboards","title":"Accessing Dashboards","text":"<p>After creating SSH tunnel:</p> <ol> <li>Open http://localhost:3000</li> <li>Login: <code>admin</code> / <code>admin</code></li> <li>Navigate to dashboards via left menu</li> </ol>"},{"location":"services/grafana/#prometheus-datasource","title":"Prometheus Datasource","text":"<p>Grafana is automatically configured to connect to Prometheus:</p> <pre><code># Auto-configured datasource\napiVersion: 1\ndatasources:\n  - name: Prometheus\n    uid: prometheus\n    type: prometheus\n    access: proxy\n    url: http://mel0210:9090\n    isDefault: true\n</code></pre>"},{"location":"services/grafana/#custom-dashboards","title":"Custom Dashboards","text":"<p>Create custom dashboards via the Grafana UI:</p> <ol> <li>Click + \u2192 Create Dashboard</li> <li>Add panels with PromQL queries</li> <li>Save dashboard</li> </ol>"},{"location":"services/grafana/#useful-queries-for-panels","title":"Useful Queries for Panels","text":"<pre><code># CPU gauge\nrate(container_cpu_usage_seconds_total{name=~\".+\"}[1m]) * 100\n\n# Memory time series\ncontainer_memory_usage_bytes{name=~\".+\"}\n\n# Network bidirectional\nrate(container_network_receive_bytes_total[1m])\n-rate(container_network_transmit_bytes_total[1m])\n</code></pre>"},{"location":"services/grafana/#environment-variables","title":"Environment Variables","text":"Variable Description Default <code>GF_SECURITY_ADMIN_USER</code> Admin username <code>admin</code> <code>GF_SECURITY_ADMIN_PASSWORD</code> Admin password <code>admin</code> <code>GF_AUTH_ANONYMOUS_ENABLED</code> Allow anonymous access <code>false</code> <code>GF_DASHBOARDS_DEFAULT_HOME_DASHBOARD_PATH</code> Home dashboard Overview"},{"location":"services/grafana/#troubleshooting","title":"Troubleshooting","text":""},{"location":"services/grafana/#cant-connect-to-prometheus","title":"Can't Connect to Prometheus","text":"<p>Check that:</p> <ol> <li>Prometheus is running (<code>python main.py --status</code>)</li> <li>Grafana's datasource URL points to correct node</li> <li>Both services are on same network</li> </ol>"},{"location":"services/grafana/#dashboards-not-loading","title":"Dashboards Not Loading","text":"<ol> <li>Check Grafana logs: <code>cat slurm-*.out</code></li> <li>Verify provisioning directory exists</li> <li>Restart Grafana service</li> </ol> <p>See also: Monitoring Overview | Prometheus</p>"},{"location":"services/mysql/","title":"MySQL Service","text":"<p>MySQL is a popular open-source relational database management system.</p>"},{"location":"services/mysql/#overview","title":"Overview","text":"Property Value Type Relational Database Default Port 3306 GPU Required No Container <code>docker://mysql:8.0</code>"},{"location":"services/mysql/#quick-start","title":"Quick Start","text":"<pre><code># Start MySQL service\npython main.py --recipe recipes/services/mysql.yaml\n\n# Run benchmark\npython main.py --recipe recipes/clients/mysql_benchmark.yaml --target-service mysql_xxx\n</code></pre>"},{"location":"services/mysql/#recipe-configuration","title":"Recipe Configuration","text":"<pre><code># recipes/services/mysql.yaml\nservice:\n  name: mysql\n  description: \"MySQL relational database\"\n\n  container:\n    docker_source: docker://mysql:8.0\n    image_path: $HOME/containers/mysql_latest.sif\n\n  resources:\n    nodes: 1\n    ntasks: 1\n    cpus_per_task: 4\n    mem: \"8G\"\n    time: \"02:00:00\"\n    partition: cpu\n    qos: default\n\n  environment:\n    MYSQL_ROOT_PASSWORD: \"benchmark_root_password\"\n    MYSQL_DATABASE: \"benchmark\"\n    MYSQL_USER: \"benchmark\"\n    MYSQL_PASSWORD: \"benchmark_password\"\n\n  ports:\n    - 3306\n</code></pre>"},{"location":"services/mysql/#with-monitoring","title":"With Monitoring","text":"<pre><code># recipes/services/mysql_with_cadvisor.yaml\nservice:\n  name: mysql\n  enable_cadvisor: true\n  cadvisor_port: 8080\n  # ... rest of config\n</code></pre>"},{"location":"services/mysql/#benchmark-client","title":"Benchmark Client","text":"<pre><code># recipes/clients/mysql_benchmark.yaml\nclient:\n  name: mysql_benchmark\n  type: mysql_benchmark\n\n  parameters:\n    database: \"benchmark\"\n    num_threads: 10\n    num_transactions: 10000\n    table_size: 100000\n    operations: \"read,write,update,delete\"\n    output_file: \"$HOME/results/mysql_benchmark.json\"\n</code></pre>"},{"location":"services/mysql/#benchmark-metrics","title":"Benchmark Metrics","text":"Metric Description <code>transactions_per_second</code> TPS <code>queries_per_second</code> QPS <code>read_latency_avg</code> Average read time <code>write_latency_avg</code> Average write time <code>connection_time</code> Connection establishment time"},{"location":"services/mysql/#cli-access","title":"CLI Access","text":"<pre><code># Create SSH tunnel\nssh -L 3306:mel0222:3306 -N u103227@login.lxp.lu -p 8822\n\n# Connect with mysql client\nmysql -h 127.0.0.1 -u benchmark -p benchmark\n</code></pre>"},{"location":"services/mysql/#performance-tuning","title":"Performance Tuning","text":"<pre><code>environment:\n  MYSQL_INNODB_BUFFER_POOL_SIZE: \"4G\"\n  MYSQL_MAX_CONNECTIONS: \"200\"\n  MYSQL_INNODB_LOG_FILE_SIZE: \"256M\"\n</code></pre> <p>See also: Services Overview</p>"},{"location":"services/ollama/","title":"Ollama Service","text":"<p>Ollama provides high-performance LLM inference with GPU acceleration.</p>"},{"location":"services/ollama/#overview","title":"Overview","text":"Property Value Type LLM Inference Default Port 11434 GPU Required Yes Container <code>docker://ollama/ollama:latest</code>"},{"location":"services/ollama/#quick-start","title":"Quick Start","text":"<pre><code># Start Ollama service\npython main.py --recipe recipes/services/ollama.yaml\n\n# Check status\npython main.py --status\n\n# Run benchmark\npython main.py --recipe recipes/clients/ollama_benchmark.yaml --target-service ollama_xxx\n</code></pre>"},{"location":"services/ollama/#recipe-configuration","title":"Recipe Configuration","text":""},{"location":"services/ollama/#basic-recipe","title":"Basic Recipe","text":"<pre><code># recipes/services/ollama.yaml\nservice:\n  name: ollama\n  description: \"Ollama LLM inference server\"\n\n  container:\n    docker_source: docker://ollama/ollama:latest\n    image_path: $HOME/containers/ollama_latest.sif\n\n  resources:\n    nodes: 1\n    ntasks: 1\n    cpus_per_task: 4\n    mem: \"32G\"\n    time: \"04:00:00\"\n    partition: gpu\n    qos: default\n    gres: \"gpu:1\"\n\n  environment:\n    OLLAMA_HOST: \"0.0.0.0:11434\"\n    OLLAMA_MODELS: \"$HOME/.ollama/models\"\n    OLLAMA_NUM_PARALLEL: \"4\"\n\n  ports:\n    - 11434\n</code></pre>"},{"location":"services/ollama/#with-monitoring","title":"With Monitoring","text":"<pre><code># recipes/services/ollama_with_cadvisor.yaml\nservice:\n  name: ollama\n  # ... same as above ...\n\n  enable_cadvisor: true\n  cadvisor_port: 8080\n</code></pre>"},{"location":"services/ollama/#environment-variables","title":"Environment Variables","text":"Variable Description Default <code>OLLAMA_HOST</code> Bind address and port <code>0.0.0.0:11434</code> <code>OLLAMA_MODELS</code> Model storage directory <code>$HOME/.ollama/models</code> <code>OLLAMA_NUM_PARALLEL</code> Concurrent requests <code>4</code> <code>OLLAMA_NUM_GPU</code> GPUs to use All available <code>OLLAMA_GPU_LAYERS</code> Layers to offload to GPU All"},{"location":"services/ollama/#supported-models","title":"Supported Models","text":"<p>Models are pulled automatically on first use:</p> Model Size Description <code>llama2</code> 3.8GB Meta's Llama 2 <code>llama2:13b</code> 7.4GB Llama 2 13B <code>codellama</code> 3.8GB Code-focused Llama <code>mistral</code> 4.1GB Mistral 7B <code>qwen2.5:0.5b</code> 0.4GB Qwen 0.5B (fast)"},{"location":"services/ollama/#api-endpoints","title":"API Endpoints","text":"<p>Once running, Ollama exposes:</p> Endpoint Method Description <code>/api/generate</code> POST Generate text completion <code>/api/chat</code> POST Chat completion <code>/api/tags</code> GET List available models <code>/api/pull</code> POST Pull a model <code>/api/embeddings</code> POST Generate embeddings"},{"location":"services/ollama/#example-generate-text","title":"Example: Generate Text","text":"<pre><code>curl http://mel2073:11434/api/generate -d '{\n  \"model\": \"llama2\",\n  \"prompt\": \"What is machine learning?\",\n  \"stream\": false\n}'\n</code></pre>"},{"location":"services/ollama/#example-chat","title":"Example: Chat","text":"<pre><code>curl http://mel2073:11434/api/chat -d '{\n  \"model\": \"llama2\",\n  \"messages\": [\n    {\"role\": \"user\", \"content\": \"Hello!\"}\n  ]\n}'\n</code></pre>"},{"location":"services/ollama/#benchmark-client","title":"Benchmark Client","text":"<p>The Ollama benchmark client tests inference performance:</p> <pre><code># recipes/clients/ollama_benchmark.yaml\nclient:\n  name: ollama_benchmark\n  type: ollama_benchmark\n\n  parameters:\n    model: \"llama2\"\n    num_requests: 50\n    concurrent_requests: 5\n    prompt_file: \"prompts.txt\"  # Optional\n    output_file: \"$HOME/results/ollama_benchmark.json\"\n\n  resources:\n    cpus_per_task: 2\n    mem: \"4G\"\n    time: \"00:30:00\"\n    partition: cpu\n</code></pre>"},{"location":"services/ollama/#benchmark-metrics","title":"Benchmark Metrics","text":"Metric Description <code>requests_per_second</code> Throughput <code>tokens_per_second</code> Generation speed <code>latency_mean</code> Average response time <code>latency_p95</code> 95th percentile latency <code>latency_p99</code> 99th percentile latency <code>success_rate</code> Percentage of successful requests"},{"location":"services/ollama/#multi-gpu-configuration","title":"Multi-GPU Configuration","text":"<p>For larger models or higher throughput:</p> <pre><code>resources:\n  gres: \"gpu:4\"  # Request 4 GPUs\n\nenvironment:\n  OLLAMA_NUM_GPU: \"4\"\n</code></pre>"},{"location":"services/ollama/#troubleshooting","title":"Troubleshooting","text":""},{"location":"services/ollama/#model-not-loading","title":"Model Not Loading","text":"<pre><code># Check GPU availability\nnvidia-smi\n\n# Check Ollama logs\ncat slurm-*.out | grep -i error\n\n# Verify model exists\ncurl http://localhost:11434/api/tags\n</code></pre>"},{"location":"services/ollama/#out-of-memory","title":"Out of Memory","text":"<ul> <li>Reduce <code>OLLAMA_NUM_PARALLEL</code></li> <li>Use a smaller model</li> <li>Request more GPU memory in resources</li> </ul>"},{"location":"services/ollama/#connection-refused","title":"Connection Refused","text":"<ul> <li>Verify <code>OLLAMA_HOST</code> is set to <code>0.0.0.0:11434</code></li> <li>Check firewall/network settings</li> <li>Ensure service is in RUNNING state</li> </ul> <p>See also: Services Overview | Benchmark Examples</p>"},{"location":"services/overview/","title":"Services Overview","text":"<p>The orchestrator supports multiple AI and database services, each optimized for HPC deployment via Apptainer containers.</p>"},{"location":"services/overview/#available-services","title":"Available Services","text":"Service Type Default Port GPU Required Description Ollama LLM Inference 11434 Yes High-performance LLM server Redis In-Memory DB 6379 No Key-value store with persistence Chroma Vector DB 8000 No Vector similarity search MySQL RDBMS 3306 No Relational database Prometheus Monitoring 9090 No Metrics collection Grafana Visualization 3000 No Dashboard visualization"},{"location":"services/overview/#service-architecture","title":"Service Architecture","text":"<p>Each service follows a common pattern:</p> <pre><code>graph LR\n    subgraph SLURM[\"SLURM Job\"]\n        subgraph Container[\"Apptainer Container\"]\n            Service[\"Service Process\"]\n        end\n        cAdvisor[\"cAdvisor&lt;br/&gt;(optional)\"]\n    end\n\n    Client[\"Benchmark Client\"] --&gt; Service\n    cAdvisor --&gt; Prometheus[\"Prometheus\"]\n    Prometheus --&gt; Grafana[\"Grafana\"]\n\n    style Service fill:#C8E6C9\n    style cAdvisor fill:#FFE0B2</code></pre>"},{"location":"services/overview/#common-configuration","title":"Common Configuration","text":"<p>All services share these common recipe fields:</p> <pre><code>service:\n  name: service_name\n  description: \"Service description\"\n\n  # Container configuration\n  container:\n    docker_source: docker://image:tag\n    image_path: $HOME/containers/image.sif\n\n  # SLURM resources\n  resources:\n    nodes: 1\n    ntasks: 1\n    cpus_per_task: 4\n    mem: \"16G\"\n    time: \"02:00:00\"\n    partition: gpu  # or cpu\n    qos: default\n\n  # Environment variables\n  environment:\n    VAR_NAME: \"value\"\n\n  # Exposed ports\n  ports:\n    - 8080\n\n  # Optional monitoring\n  enable_cadvisor: true\n  cadvisor_port: 8080\n</code></pre>"},{"location":"services/overview/#service-lifecycle","title":"Service Lifecycle","text":"<pre><code>stateDiagram-v2\n    [*] --&gt; Recipe: Load YAML\n    Recipe --&gt; Script: Generate SLURM script\n    Script --&gt; Submitted: sbatch\n    Submitted --&gt; Pending: Queued\n    Pending --&gt; Starting: Resources allocated\n    Starting --&gt; Running: Container ready\n    Running --&gt; Running: Health checks pass\n    Running --&gt; Stopped: scancel or timeout\n    Stopped --&gt; [*]</code></pre>"},{"location":"services/overview/#monitoring-integration","title":"Monitoring Integration","text":"<p>Services can be deployed with cAdvisor for container metrics:</p> <pre><code>service:\n  name: ollama\n  enable_cadvisor: true\n  cadvisor_port: 8080\n</code></pre> <p>This enables collection of:</p> <ul> <li>CPU Usage: <code>container_cpu_usage_seconds_total</code></li> <li>Memory Usage: <code>container_memory_usage_bytes</code></li> <li>Network I/O: <code>container_network_*_bytes_total</code></li> <li>Filesystem: <code>container_fs_usage_bytes</code></li> </ul>"},{"location":"services/overview/#quick-start-commands","title":"Quick Start Commands","text":"<pre><code># List available services\npython main.py --list-services\n\n# Start a service\npython main.py --recipe recipes/services/ollama.yaml\n\n# Check status\npython main.py --status\n\n# Stop a service\npython main.py --stop-service &lt;service_id&gt;\n</code></pre>"},{"location":"services/overview/#service-specific-documentation","title":"Service-Specific Documentation","text":"<ul> <li>Ollama (LLM) - GPU-accelerated language model inference</li> <li>Redis - In-memory database with benchmarking</li> <li>Chroma - Vector similarity search</li> <li>MySQL - Relational database</li> <li>Prometheus - Metrics collection and storage</li> <li>Grafana - Visualization and dashboards</li> </ul>"},{"location":"services/prometheus/","title":"Prometheus Service","text":"<p>Prometheus is a monitoring and alerting toolkit for collecting and querying metrics.</p>"},{"location":"services/prometheus/#overview","title":"Overview","text":"Property Value Type Monitoring Default Port 9090 GPU Required No Container <code>docker://prom/prometheus:latest</code>"},{"location":"services/prometheus/#quick-start","title":"Quick Start","text":"<pre><code># Start Prometheus with service monitoring\npython main.py --recipe recipes/services/prometheus_with_cadvisor.yaml\n\n# Create tunnel\nssh -L 9090:mel0210:9090 -N u103227@login.lxp.lu -p 8822\n\n# Access UI\nopen http://localhost:9090\n</code></pre>"},{"location":"services/prometheus/#recipe-configuration","title":"Recipe Configuration","text":"<pre><code># recipes/services/prometheus_with_cadvisor.yaml\nservice:\n  name: prometheus\n  description: \"Prometheus monitoring\"\n\n  container:\n    docker_source: docker://prom/prometheus:latest\n    image_path: $HOME/containers/prometheus.sif\n\n  resources:\n    nodes: 1\n    cpus_per_task: 2\n    mem: \"4G\"\n    time: \"02:00:00\"\n    partition: cpu\n\n  # Services to monitor\n  monitoring_targets:\n    - service_id: \"ollama_xxx\"\n      job_name: \"ollama-cadvisor\"\n      port: 8080\n    - service_id: \"redis_xxx\"\n      job_name: \"redis-cadvisor\"\n      port: 8080\n\n  environment:\n    PROMETHEUS_RETENTION_TIME: \"15d\"\n\n  ports:\n    - 9090\n</code></pre>"},{"location":"services/prometheus/#configuration","title":"Configuration","text":"<p>Prometheus is automatically configured to scrape cAdvisor endpoints from monitored services.</p>"},{"location":"services/prometheus/#auto-generated-config","title":"Auto-Generated Config","text":"<pre><code># $HOME/prometheus/config/prometheus.yml\nglobal:\n  scrape_interval: 15s\n  evaluation_interval: 15s\n\nscrape_configs:\n  - job_name: 'prometheus'\n    static_configs:\n      - targets: ['localhost:9090']\n\n  - job_name: 'ollama-cadvisor'\n    static_configs:\n      - targets: ['mel2073:8080']\n        labels:\n          service: 'ollama'\n          instance: 'mel2073'\n\n  - job_name: 'redis-cadvisor'\n    static_configs:\n      - targets: ['mel0182:8080']\n        labels:\n          service: 'redis'\n          instance: 'mel0182'\n</code></pre>"},{"location":"services/prometheus/#useful-promql-queries","title":"Useful PromQL Queries","text":""},{"location":"services/prometheus/#cpu-usage","title":"CPU Usage","text":"<pre><code># CPU usage rate by container\nrate(container_cpu_usage_seconds_total{name=~\".+\"}[5m])\n\n# Average CPU usage\navg(rate(container_cpu_usage_seconds_total[5m]))\n</code></pre>"},{"location":"services/prometheus/#memory-usage","title":"Memory Usage","text":"<pre><code># Memory usage by container\ncontainer_memory_usage_bytes{name=~\".+\"}\n\n# Memory working set\ncontainer_memory_working_set_bytes{name=~\".+\"}\n</code></pre>"},{"location":"services/prometheus/#network-traffic","title":"Network Traffic","text":"<pre><code># Network receive rate\nrate(container_network_receive_bytes_total[5m])\n\n# Network transmit rate\nrate(container_network_transmit_bytes_total[5m])\n</code></pre>"},{"location":"services/prometheus/#container-info","title":"Container Info","text":"<pre><code># Active containers\ncount(container_last_seen{name=~\".+\"})\n\n# Scrape targets up\nup\n</code></pre>"},{"location":"services/prometheus/#querying-via-cli","title":"Querying via CLI","text":"<pre><code># Query metrics from orchestrator\npython main.py --query-metrics prometheus_xxx \"container_memory_usage_bytes\"\n\n# With labels\npython main.py --query-metrics prometheus_xxx 'container_memory_usage_bytes{name=\"ollama\"}'\n</code></pre>"},{"location":"services/prometheus/#data-retention","title":"Data Retention","text":"<p>Configure retention in environment:</p> <pre><code>environment:\n  PROMETHEUS_RETENTION_TIME: \"7d\"    # Keep 7 days\n  PROMETHEUS_STORAGE_PATH: \"/prometheus\"\n</code></pre> <p>See also: Monitoring Overview | Grafana</p>"},{"location":"services/redis/","title":"Redis Service","text":"<p>Redis is an in-memory data structure store, used as a database, cache, and message broker.</p>"},{"location":"services/redis/#overview","title":"Overview","text":"Property Value Type In-Memory Database Default Port 6379 GPU Required No Container <code>docker://redis:latest</code>"},{"location":"services/redis/#quick-start","title":"Quick Start","text":"<pre><code># Start Redis service\npython main.py --recipe recipes/services/redis.yaml\n\n# Run benchmark\npython main.py --recipe recipes/clients/redis_benchmark.yaml --target-service redis_xxx\n\n# Download results\npython main.py --download-results\n</code></pre>"},{"location":"services/redis/#recipe-configuration","title":"Recipe Configuration","text":""},{"location":"services/redis/#basic-recipe","title":"Basic Recipe","text":"<pre><code># recipes/services/redis.yaml\nservice:\n  name: redis\n  description: \"Redis in-memory database\"\n\n  container:\n    docker_source: docker://redis:latest\n    image_path: $HOME/containers/redis_latest.sif\n\n  resources:\n    nodes: 1\n    ntasks: 1\n    cpus_per_task: 4\n    mem: \"8G\"\n    time: \"02:00:00\"\n    partition: cpu\n    qos: default\n\n  environment:\n    REDIS_PORT: \"6379\"\n\n  ports:\n    - 6379\n</code></pre>"},{"location":"services/redis/#with-persistence","title":"With Persistence","text":"<pre><code>service:\n  name: redis\n\n  environment:\n    REDIS_PORT: \"6379\"\n    REDIS_APPENDONLY: \"yes\"\n    REDIS_APPENDFSYNC: \"everysec\"\n\n  # Bind data directory for persistence\n  bind_mounts:\n    - \"$HOME/redis/data:/data\"\n</code></pre>"},{"location":"services/redis/#with-monitoring","title":"With Monitoring","text":"<pre><code># recipes/services/redis_with_cadvisor.yaml\nservice:\n  name: redis\n  enable_cadvisor: true\n  cadvisor_port: 8080\n  # ... rest of config\n</code></pre>"},{"location":"services/redis/#configuration-options","title":"Configuration Options","text":"Option Description Default <code>maxmemory</code> Maximum memory limit No limit <code>maxmemory-policy</code> Eviction policy <code>noeviction</code> <code>appendonly</code> Enable AOF persistence <code>no</code> <code>save</code> RDB snapshot intervals Disabled"},{"location":"services/redis/#benchmark-client","title":"Benchmark Client","text":""},{"location":"services/redis/#single-run-benchmark","title":"Single-Run Benchmark","text":"<pre><code># recipes/clients/redis_benchmark.yaml\nclient:\n  name: redis_benchmark\n  type: redis_benchmark\n\n  parameters:\n    clients: 50\n    requests: 100000\n    data_size: 256\n    tests: \"SET,GET,LPUSH,LPOP,SADD\"\n    output_file: \"$HOME/results/redis_benchmark.json\"\n</code></pre>"},{"location":"services/redis/#parametric-benchmark","title":"Parametric Benchmark","text":"<p>Run comprehensive sweeps across multiple parameters:</p> <pre><code># recipes/clients/redis_parametric.yaml\nclient:\n  name: redis_parametric\n  type: redis_parametric_benchmark\n\n  parameters:\n    client_counts: [1, 10, 50, 100, 200]\n    data_sizes: [64, 256, 1024, 4096]\n    pipeline_depths: [1, 10, 50]\n    tests: \"SET,GET\"\n</code></pre> <p>Run with:</p> <pre><code>./scripts/run_redis_parametric.sh\n</code></pre>"},{"location":"services/redis/#benchmark-metrics","title":"Benchmark Metrics","text":"Metric Description <code>ops_per_second</code> Operations per second <code>latency_avg</code> Average latency (ms) <code>latency_p50</code> Median latency <code>latency_p95</code> 95th percentile <code>latency_p99</code> 99th percentile"},{"location":"services/redis/#cli-operations","title":"CLI Operations","text":"<p>Connect to Redis via <code>redis-cli</code>:</p> <pre><code># Via SSH tunnel\nssh -L 6379:mel0182:6379 -N u103227@login.lxp.lu -p 8822\n\n# Then locally\nredis-cli\n&gt; SET key \"value\"\n&gt; GET key\n&gt; INFO\n</code></pre>"},{"location":"services/redis/#performance-tuning","title":"Performance Tuning","text":""},{"location":"services/redis/#high-throughput","title":"High Throughput","text":"<pre><code>environment:\n  REDIS_TCP_BACKLOG: \"511\"\n  REDIS_MAXCLIENTS: \"10000\"\n</code></pre>"},{"location":"services/redis/#memory-optimization","title":"Memory Optimization","text":"<pre><code>environment:\n  REDIS_MAXMEMORY: \"4gb\"\n  REDIS_MAXMEMORY_POLICY: \"allkeys-lru\"\n</code></pre>"},{"location":"services/redis/#results-analysis","title":"Results Analysis","text":"<p>After running parametric benchmarks, analyze with:</p> <pre><code># Generate plots\npython analysis/plot_redis_results.py\n\n# View summary\ncat results/redis_parametric_summary.json | jq .\n</code></pre> <p>Generated plots include:</p> <ul> <li>Throughput vs. clients</li> <li>Latency distribution</li> <li>Data size impact</li> <li>Pipeline depth analysis</li> </ul> <p>See also: Services Overview | Parametric Benchmarks</p>"}]}